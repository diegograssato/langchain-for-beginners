{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Componentes Fundamentais:\n",
    "\n",
    "- **Models (Modelos):** LangChain serve como uma interface padrão que permite interações com uma ampla gama de Grandes Modelos de Linguagem (LLMs).\n",
    "\n",
    "- **Chains (Cadeias):** Como seu nome implica, _cadeias_ são o núcleo dos fluxos de trabalho do LangChain. Combinam LLMs com outros componentes, criando aplicativos por meio da execução de uma sequência de funções.\n",
    "\n",
    "- **Prompts (Instruções):** Os prompts são as instruções apresentadas a um LLM. Geralmente, a \"arte\" de redigir prompts que efetivamente entregam o contexto necessário para que o LLM interprete a entrada e a saída da estrutura da maneira mais útil para você é chamada de engenharia de prompt.\n",
    "\n",
    "- **Indexes (Índices):** Para realizar determinadas tarefas, as LLMs precisarão acessar fontes de dados externas específicas não incluídas em seu conjunto de dados de treinamento, como documentos internos, e-mails ou conjuntos de dados. LangChain refere-se coletivamente a essa documentação externa como “índices\".\"\n",
    "\n",
    "- **Memory (Memória):** Por padrão, os LLMs não têm memória de longo prazo de conversas anteriores (a menos que o histórico do chat seja usado como entrada para uma consulta). O LangChain soluciona esse problema com utilitários simples para adicionar memória a um sistema, com opções que vão desde a retenção total de todas as conversas até a retenção de um resumo da conversa até a retenção das _n_ trocas mais recentes.\n",
    "\n",
    "- **Agents/Tools (Agentes/Ferramentas):** Os agentes do LangChain podem usar um determinado modelo LLM como um \"mecanismo de raciocínio\" para determinar quais ações tomar. Ao criar uma cadeia para um agente, as entradas contêm:\n",
    "\n",
    "\t- uma lista de ferramentas disponíveis para serem aproveitadas.\n",
    "\t- entrada do usuário (como prompts e consultas).\n",
    "\t- quaisquer etapas relevantes executadas anteriormente.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../helpers/00-llm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Importa conectando com o cliente OpenAI\n",
    "from helpers.llm import initialize_llm, logger, pretty_print\n",
    "\n",
    "# Parte 1 - Importando os componentes do LangChain\n",
    "llm, _, _ = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=========== Exemplo 1 =========\")\n",
    "prompt_template = PromptTemplate.from_template(\"Gere para mim um poema sobre: {assunto}. Escreva em {lingua}\")\n",
    "retorno = prompt_template.invoke({\"assunto\": \"navegação\", \"lingua\":\"pt-br\"})\n",
    "\n",
    "pretty_print(retorno)\n",
    "print(\"-------------------------------------------------------------------\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=========== Exemplo 3 =========\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "\t\t\t\t\t\t\t\t\t  (\"system\", \"Você é um assistente de IA com habilidade de escritor de poesia.\"),\n",
    "\t\t\t\t\t\t\t\t\t  (\"user\", \"Gere para mim um poema sobre: {assunto}. Escreva em {lingua}\")\n",
    "])\n",
    "\n",
    "retorno = prompt_template.invoke({\"assunto\": \"navegação\", \"lingua\":\"pt-br\"})\n",
    "pretty_print(retorno)\n",
    "print(\"-------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exemplo 1:\n",
    "# PART 1: Criando ChatPromptTemplate\n",
    "print(\"-----Exemplo Chain 1 -----\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"user\", \"Escreva um poema em {lingua} sobre o tema: {assunto}\")\n",
    "]\n",
    ")\n",
    "\n",
    "# PART 2: Criando a chain\n",
    "chain1 = prompt_template | llm\n",
    "\n",
    "# PART 3: Invoke da chain passando as variáveis.\n",
    "resposta = chain1.invoke({\"lingua\": \"pt-br\", \"assunto\":\"frutas\"})\n",
    "\n",
    "pretty_print(resposta.content)\n",
    "print(\"-------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exemplo 2:\n",
    "# PART 1: Criando ChatPromptTemplate já com mensagem de sistema:\n",
    "print(\"-----Exemplo Chain 2 -----\")\n",
    "\n",
    "mensagens = [\n",
    "    (\"system\", \"Você é um poeta brasileiro famoso e escreve poemas de no máximo {n_versos} versos.\"),\n",
    "    (\"human\", \"Escreva para mim um poema sobre {assunto}.\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate(mensagens)\n",
    "\n",
    "# PART 2: Criando a chain\n",
    "chain2 = prompt_template | llm\n",
    "\n",
    "# PART 3: Invoke da chain passando as variáveis.\n",
    "resposta = chain2.invoke({\"n_versos\": \"10\", \"assunto\":\"navios\"})\n",
    "\n",
    "pretty_print(resposta.content)\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "resposta = llm.invoke(\"Qual a capital da França?\")\n",
    "pretty_print(resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
