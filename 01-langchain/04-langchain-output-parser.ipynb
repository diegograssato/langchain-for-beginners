{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Definição:\n",
    "\n",
    "[Documentação](https://python.langchain.com/docs/concepts/output_parsers/)\n",
    "\n",
    "Os analisadores de saída são responsáveis ​​por pegar a saída de um LLM e transformá-la em um formato mais adequado. Isso é muito útil quando você está usando LLMs para gerar qualquer forma de dados estruturados.\n",
    "\n",
    "Como sabemos os modelos LLMs podem gerar respostas diferentes para uma mesma pergunta, ou seja, eles não são determinísticos. Por isso, os OutputParsers são componentes do langchain que formatam a saída do modelos em uma estrutura predefinida, seja uma simples string da resposta ou um formato JSON, por exemplo.\n",
    "\n",
    "Além de ter uma grande coleção de diferentes tipos de analisadores de saída, um benefício diferenciado do LangChain OutputParsers é que muitos deles oferecem suporte a streaming (ou seja, funcionam com a transmissão de dados token a token).\n",
    "\n",
    "Outro ponto fundamental é que os OutputParsers implementam interface `Runnables`, ou seja,  são componentes de LangChain e implementam as funções `invoke`, `ainvoke` etc. Sua entrada pode ser uma string, ou um `BaseMessage` (mensagem obtida ao invocar um modelo usando `ChatPromptTemplate`).\n",
    "\n",
    "### Contextualização\n",
    "\n",
    "Até o momento nós capturávamos as respostas do modelo dentro de uma chain acessando o conteúdo da `AIMessage` resposta. Porém há uma forma mais 'bonita' de se obter a resposta, utilizando um analisador de saída do tipo string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../helpers/00-llm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Importa conectando com o cliente OpenAI\n",
    "from helpers.llm import initialize_llm, logger, pretty_print\n",
    "\n",
    "# Parte 1 - Importando os componentes do LangChain\n",
    "llm, _, _ = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = ChatPromptTemplate([(\"user\", \"Escreva um poema em {lingua} sobre o tema: {assunto}\")])\n",
    "\n",
    "# PART 2: Criando a chain\n",
    "chain1 = prompt_template | llm\n",
    "\n",
    "# PART 3: Invoke da chain passando as variáveis.\n",
    "resposta = chain1.invoke({\"lingua\": \"pt-br\", \"assunto\":\"frutas\"})\n",
    "\n",
    "pretty_print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso a resposta será um componente `AIMessage` e estamos acessando o conteúdo da resposta por meio de `.content`. Porém você pode fazer a seguinte combinação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "analisador_saida = StrOutputParser()\n",
    "\n",
    "# PART 2: Criando a chain\n",
    "chain2 = prompt_template | llm |analisador_saida\n",
    "# PART 3: Invoke da chain passando as variáveis.\n",
    "resposta = chain2.invoke({\"lingua\": \"pt-br\", \"assunto\":\"frutas\"})\n",
    "\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ao executar, você obterá em 'resposta' o conteúdo gerado pelo LLM sem necessitar realizar o acesso pela variável `content`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PydanticOutputParser\n",
    "[Documentação](https://python.langchain.com/docs/how_to/output_parser_structured/)\n",
    "\n",
    "Os modelos de linguagem produzem texto. Mas há momentos em que você quer obter informações mais estruturadas do que apenas texto de volta. Embora alguns provedores de modelos suportem maneiras integradas de retornar saída estruturada , nem todos o fazem.\n",
    "\n",
    "Os analisadores de saída são classes que ajudam a estruturar respostas de modelos de linguagem.\n",
    "\n",
    "No caso do tipo PydanticOutputParser, é implementada uma saída estruturada usando a classe Pydantic, onde você declara quais variáveis deve ter na saída do LLM para criar uma instancia da sua classe pydantic. Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser  \n",
    "from langchain.prompts import ChatPromptTemplate  \n",
    "from pydantic import BaseModel, Field  \n",
    "   \n",
    "  \n",
    "# Definindo a minha estrutura de saída  \n",
    "class Rota(BaseModel):  \n",
    "    escolha: int = Field(description=\"Rota escolhida\")  \n",
    "    pensamento: str = Field(description=\"Campo para o pensamento que levou a decisão da rota escolhida\")  \n",
    "  \n",
    "  \n",
    "# Criando o analisador de saída  \n",
    "parser = PydanticOutputParser(pydantic_object=Rota)  \n",
    "  \n",
    "prompt_template = ChatPromptTemplate([(\"system\",  \n",
    "                                       \"Se a pergunta do usuário for relacionado ao setor financeiro, a escolha deve ser 1, caso contrário a escolha pode ser qualquer numero diferente de 1. \\n{format_instructions}\\n Pergunta Usuário: {pergunta_user}\")],  \n",
    "                                     partial_variables={\"format_instructions\": parser.get_format_instructions()})  \n",
    "  \n",
    "  \n",
    "\n",
    "prompt_and_model = prompt_template | llm  \n",
    "output = prompt_and_model.invoke({\"pergunta_user\": \"Me diga quanto está o dollar.\"})  \n",
    "pretty_print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que ele formatou a resposta entre a notação `json`. Dessa forma, é possivel agora encadear na chain o analisador de saida para que tenhamos como resposta uma instancia de `Rota`, ou seja, realizando a adaptação na chain encadeando o `parser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_and_model = prompt_template | llm | parser\n",
    "output = prompt_and_model.invoke({\"pergunta_user\": \"Me diga quanto está o dollar.\"})  \n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JsonOutputParser\n",
    "[Documentação](https://python.langchain.com/docs/how_to/output_parser_json/)\n",
    "\n",
    "O `JsonOutputParser` é uma opção interna para solicitar e então analisar a saída JSON de um modelo LLM. \n",
    "\n",
    "Pode ser implementado usando Pydantic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser  \n",
    "from langchain.prompts import ChatPromptTemplate   \n",
    "from pydantic import BaseModel, Field  \n",
    "   \n",
    "# Definindo a minha estrutura de saída  \n",
    "class Rota(BaseModel):  \n",
    "    escolha: int = Field(description=\"Rota escolhida\")  \n",
    "    pensamento: str = Field(description=\"Campo para o pensamento que levou a decisão da rota escolhida\")  \n",
    "  \n",
    "  \n",
    "# Criando o analisador de saída  \n",
    "parser = JsonOutputParser(pydantic_object=Rota)  \n",
    "  \n",
    "prompt_template = ChatPromptTemplate([(\"system\",  \n",
    "                                       \"Se a pergunta do usuário for relacionado ao setor financeiro, a escolha deve ser 1, caso contrário a escolha pode ser qualquer numero diferente de 1. \\n{format_instructions}\\n Pergunta Usuário: {pergunta_user}\")],  \n",
    "                                     partial_variables={\"format_instructions\": parser.get_format_instructions()})  \n",
    "  \n",
    "  \n",
    "\n",
    "prompt_and_model = prompt_template | llm  | parser\n",
    "\n",
    "output = prompt_and_model.invoke({\"pergunta_user\": \"Me diga quanto está o dollar.\"})  \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou não usando Pydantic:\n",
    "\n",
    "*Atenção: Você também pode usar o `JsonOutputParser`sem Pydantic. Isso fará com que o modelo retorne JSON, mas não fornece detalhes sobre qual deve ser o esquema. Ou seja, pode ser que o modelo erre a estrutura do JSON esperada.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser  \n",
    "from langchain.prompts import ChatPromptTemplate  \n",
    "   \n",
    "  \n",
    "# Criando o analisador de saída  \n",
    "parser = JsonOutputParser()\n",
    "  \n",
    "prompt_template = ChatPromptTemplate([(\"system\",  \n",
    "                                       \"Se a pergunta do usuário for relacionado ao setor financeiro, a escolha deve ser 1, caso contrário a escolha pode ser qualquer numero diferente de 1. \\n{format_instructions}\\n Pergunta Usuário: {pergunta_user}\")],  \n",
    "                                     partial_variables={\"format_instructions\": parser.get_format_instructions()})  \n",
    "  \n",
    "  \n",
    "\n",
    "prompt_and_model = prompt_template | llm  | parser\n",
    "\n",
    "output = prompt_and_model.invoke({\"pergunta_user\": \"Me diga quanto está o dollar.\"})  \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outros:\n",
    "\n",
    "- XMLOutputParser: [Documentação](https://python.langchain.com/docs/how_to/output_parser_xml/)\n",
    "- YamlOutputParser: [Documentação](https://python.langchain.com/docs/how_to/output_parser_yaml/)\n",
    "- DatetimeOutputParser:[Documentação](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html#langchain.output_parsers.datetime.DatetimeOutputParser)\n",
    "- EnumOutputParser: [Documentação](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html#langchain.output_parsers.enum.EnumOutputParser)\n",
    "- PandasDataFrameOutputParser: [Documentação](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html#langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (grassato)",
   "language": "python",
   "name": "grassato"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
