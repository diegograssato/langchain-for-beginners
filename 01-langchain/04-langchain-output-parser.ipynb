{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Definição:\n",
    "\n",
    "[Documentação](https://python.langchain.com/docs/concepts/output_parsers/)\n",
    "\n",
    "Os analisadores de saída são responsáveis ​​por pegar a saída de um LLM e transformá-la em um formato mais adequado. Isso é muito útil quando você está usando LLMs para gerar qualquer forma de dados estruturados.\n",
    "\n",
    "Como sabemos os modelos LLMs podem gerar respostas diferentes para uma mesma pergunta, ou seja, eles não são determinísticos. Por isso, os OutputParsers são componentes do langchain que formatam a saída do modelos em uma estrutura predefinida, seja uma simples string da resposta ou um formato JSON, por exemplo.\n",
    "\n",
    "Além de ter uma grande coleção de diferentes tipos de analisadores de saída, um benefício diferenciado do LangChain OutputParsers é que muitos deles oferecem suporte a streaming (ou seja, funcionam com a transmissão de dados token a token).\n",
    "\n",
    "Outro ponto fundamental é que os OutputParsers implementam interface `Runnables`, ou seja,  são componentes de LangChain e implementam as funções `invoke`, `ainvoke` etc. Sua entrada pode ser uma string, ou um `BaseMessage` (mensagem obtida ao invocar um modelo usando `ChatPromptTemplate`).\n",
    "\n",
    "### Contextualização\n",
    "\n",
    "Até o momento nós capturávamos as respostas do modelo dentro de uma chain acessando o conteúdo da `AIMessage` resposta. Porém há uma forma mais 'bonita' de se obter a resposta, utilizando um analisador de saída do tipo string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../helpers/00-llm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:helpers.llm:Using AzureOpenAI.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Importa conectando com o cliente OpenAI\n",
    "from helpers.llm import initialize_llm, logger, pretty_print\n",
    "\n",
    "# Parte 1 - Importando os componentes do LangChain\n",
    "llm, _, _ = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://trainning-grassato-01.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_template = ChatPromptTemplate([(\"user\", \"Escreva um poema em {lingua} sobre o tema: {assunto}\")])\n",
    "\n",
    "# PART 2: Criando a chain\n",
    "chain1 = prompt_template | llm\n",
    "\n",
    "# PART 3: Invoke da chain passando as variáveis.\n",
    "resposta = chain1.invoke({\"lingua\": \"pt-br\", \"assunto\":\"frutas\"})\n",
    "\n",
    "print(type(resposta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso a resposta será um componente `AIMessage` e estamos acessando o conteúdo da resposta por meio de `.content`. Porém você pode fazer a seguinte combinação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://trainning-grassato-01.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pomar da manhã, o sol desperta,  \n",
      "Entre folhas e galhos, a vida é aberta.  \n",
      "Frutas pendem, promessas de cor,  \n",
      "Doces segredos, perfumes de amor.\n",
      "\n",
      "A manga dourada, suculenta e macia,  \n",
      "Escorre nos dedos, pura alegria.  \n",
      "A maçã reluzente, de brilho sutil,  \n",
      "Guarda em seu ventre um sabor infantil.\n",
      "\n",
      "Banana se curva, risonha e amável,  \n",
      "No cacho dourado, é sempre agradável.  \n",
      "O abacaxi, com coroa real,  \n",
      "Traz acidez e doçura no mesmo final.\n",
      "\n",
      "Morango pequeno, vermelho paixão,  \n",
      "Desperta desejos no coração.  \n",
      "A uva se esconde, tímida, em cacho,  \n",
      "Mas explode em festa quando eu a acho.\n",
      "\n",
      "Frutas são risos da terra e do céu,  \n",
      "Mistura de cores num grande painel.  \n",
      "No prato, na boca, no cheiro, no ar,  \n",
      "Frutas são versos prontos pra encantar.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "analisador_saida = StrOutputParser()\n",
    "\n",
    "# PART 2: Criando a chain\n",
    "chain2 = prompt_template | llm |analisador_saida\n",
    "# PART 3: Invoke da chain passando as variáveis.\n",
    "resposta = chain2.invoke({\"lingua\": \"pt-br\", \"assunto\":\"frutas\"})\n",
    "\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ao executar, você obterá em 'resposta' o conteúdo gerado pelo LLM sem necessitar realizar o acesso pela variável `content`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PydanticOutputParser\n",
    "[Documentação](https://python.langchain.com/docs/how_to/output_parser_structured/)\n",
    "\n",
    "Os modelos de linguagem produzem texto. Mas há momentos em que você quer obter informações mais estruturadas do que apenas texto de volta. Embora alguns provedores de modelos suportem maneiras integradas de retornar saída estruturada , nem todos o fazem.\n",
    "\n",
    "Os analisadores de saída são classes que ajudam a estruturar respostas de modelos de linguagem.\n",
    "\n",
    "No caso do tipo PydanticOutputParser, é implementada uma saída estruturada usando a classe Pydantic, onde você declara quais variáveis deve ter na saída do LLM para criar uma instancia da sua classe pydantic. Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://trainning-grassato-01.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "{\"escolha\": 1, \"pensamento\": \"A pergunta do usuário está relacionada ao setor financeiro, pois envolve a cotação do dólar, que é um tema financeiro.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser  \n",
    "from langchain.prompts import ChatPromptTemplate  \n",
    "from pydantic import BaseModel, Field  \n",
    "   \n",
    "  \n",
    "# Definindo a minha estrutura de saída  \n",
    "class Rota(BaseModel):  \n",
    "    escolha: int = Field(description=\"Rota escolhida\")  \n",
    "    pensamento: str = Field(description=\"Campo para o pensamento que levou a decisão da rota escolhida\")  \n",
    "  \n",
    "  \n",
    "# Criando o analisador de saída  \n",
    "parser = PydanticOutputParser(pydantic_object=Rota)  \n",
    "  \n",
    "prompt_template = ChatPromptTemplate([(\"system\",  \n",
    "                                       \"Se a pergunta do usuário for relacionado ao setor financeiro, a escolha deve ser 1, caso contrário a escolha pode ser qualquer numero diferente de 1. \\n{format_instructions}\\n Pergunta Usuário: {pergunta_user}\")],  \n",
    "                                     partial_variables={\"format_instructions\": parser.get_format_instructions()})  \n",
    "  \n",
    "  \n",
    "\n",
    "prompt_and_model = prompt_template | llm  \n",
    "output = prompt_and_model.invoke({\"pergunta_user\": \"Me diga quanto está o dollar.\"})  \n",
    "pretty_print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que ele formatou a resposta entre a notação `json`. Dessa forma, é possivel agora encadear na chain o analisador de saida para que tenhamos como resposta uma instancia de `Rota`, ou seja, realizando a adaptação na chain encadeando o `parser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://trainning-grassato-01.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "escolha=1 pensamento='A pergunta do usuário é sobre a cotação do dólar, que é um tema diretamente relacionado ao setor financeiro. Portanto, a escolha deve ser 1.'\n"
     ]
    }
   ],
   "source": [
    "prompt_and_model = prompt_template | llm | parser\n",
    "output = prompt_and_model.invoke({\"pergunta_user\": \"Me diga quanto está o dollar.\"})  \n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JsonOutputParser\n",
    "[Documentação](https://python.langchain.com/docs/how_to/output_parser_json/)\n",
    "\n",
    "O `JsonOutputParser` é uma opção interna para solicitar e então analisar a saída JSON de um modelo LLM. \n",
    "\n",
    "Pode ser implementado usando Pydantic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://trainning-grassato-01.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'escolha': 1, 'pensamento': 'A pergunta do usuário é sobre a cotação do dólar, que está diretamente relacionada ao setor financeiro. Portanto, a escolha correta é 1.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser  \n",
    "from langchain.prompts import ChatPromptTemplate   \n",
    "from pydantic import BaseModel, Field  \n",
    "   \n",
    "# Definindo a minha estrutura de saída  \n",
    "class Rota(BaseModel):  \n",
    "    escolha: int = Field(description=\"Rota escolhida\")  \n",
    "    pensamento: str = Field(description=\"Campo para o pensamento que levou a decisão da rota escolhida\")  \n",
    "  \n",
    "  \n",
    "# Criando o analisador de saída  \n",
    "parser = JsonOutputParser(pydantic_object=Rota)  \n",
    "  \n",
    "prompt_template = ChatPromptTemplate([(\"system\",  \n",
    "                                       \"Se a pergunta do usuário for relacionado ao setor financeiro, a escolha deve ser 1, caso contrário a escolha pode ser qualquer numero diferente de 1. \\n{format_instructions}\\n Pergunta Usuário: {pergunta_user}\")],  \n",
    "                                     partial_variables={\"format_instructions\": parser.get_format_instructions()})  \n",
    "  \n",
    "  \n",
    "\n",
    "prompt_and_model = prompt_template | llm  | parser\n",
    "\n",
    "output = prompt_and_model.invoke({\"pergunta_user\": \"Me diga quanto está o dollar.\"})  \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou não usando Pydantic:\n",
    "\n",
    "*Atenção: Você também pode usar o `JsonOutputParser`sem Pydantic. Isso fará com que o modelo retorne JSON, mas não fornece detalhes sobre qual deve ser o esquema. Ou seja, pode ser que o modelo erre a estrutura do JSON esperada.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://trainning-grassato-01.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'escolha': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser  \n",
    "from langchain.prompts import ChatPromptTemplate  \n",
    "   \n",
    "  \n",
    "# Criando o analisador de saída  \n",
    "parser = JsonOutputParser()\n",
    "  \n",
    "prompt_template = ChatPromptTemplate([(\"system\",  \n",
    "                                       \"Se a pergunta do usuário for relacionado ao setor financeiro, a escolha deve ser 1, caso contrário a escolha pode ser qualquer numero diferente de 1. \\n{format_instructions}\\n Pergunta Usuário: {pergunta_user}\")],  \n",
    "                                     partial_variables={\"format_instructions\": parser.get_format_instructions()})  \n",
    "  \n",
    "  \n",
    "\n",
    "prompt_and_model = prompt_template | llm  | parser\n",
    "\n",
    "output = prompt_and_model.invoke({\"pergunta_user\": \"Me diga quanto está o dollar.\"})  \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outros:\n",
    "\n",
    "- XMLOutputParser: [Documentação](https://python.langchain.com/docs/how_to/output_parser_xml/)\n",
    "- YamlOutputParser: [Documentação](https://python.langchain.com/docs/how_to/output_parser_yaml/)\n",
    "- DatetimeOutputParser:[Documentação](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html#langchain.output_parsers.datetime.DatetimeOutputParser)\n",
    "- EnumOutputParser: [Documentação](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html#langchain.output_parsers.enum.EnumOutputParser)\n",
    "- PandasDataFrameOutputParser: [Documentação](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html#langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
