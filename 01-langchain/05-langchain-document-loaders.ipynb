{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição:\n",
    "Os carregadores de documentos são componentes do LangChain que facilitam o carregamentos de arquivos/documentos para que eles possam interagir  com outros componentes de LangChain.\n",
    "Eles não são utilizados diretamente dentro das cadeias (chains), mas são peças fundamentais no pré processamento dos dados para que você possa adicionar informações à sua chain (mais especifico nos prompts) ou indexar informações em bancos vetoriais.\n",
    "\n",
    "Os carregadores de documentos são projetados para carregar objetos de documentos. O LangChain tem centenas de integrações com várias fontes de dados para carregar dados de: Slack, Notion, Google Drive, etc. ([link](https://python.langchain.com/docs/integrations/document_loaders/))\n",
    "\n",
    "Cada Document Loader tem seus próprios parâmetros específicos, mas todos eles podem ser invocados da mesma maneira com o método `.load`(método padrão de carregamento) ou `.lazy_load` (usado para trabalhar com grandes conjuntos de dados).\n",
    "[Documentação](https://python.langchain.com/docs/how_to/#document-loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos\n",
    "\n",
    "##### Carregadores de PDF:\n",
    "\n",
    "[link](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n",
    "#### Simples Carregamento de texto:\n",
    "\n",
    "Para fazer a leitura de texto presente em PDFs, podemos usar a biblioteca pypdf.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versão assíncrona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../helpers/00-llm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa conectando com o cliente OpenAI\n",
    "from helpers.llm import initialize_llm, logger, pretty_print_docs\n",
    "\n",
    "# Parte 1 - Importando os componentes do LangChain\n",
    "llm, _, _ = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader  \n",
    "import asyncio  # Import necessário para gerenciar o loop de eventos  \n",
    "  \n",
    "  \n",
    "# Define uma função assíncrona para realizar a tarefa  \n",
    "async def process_pdf():  \n",
    "    loader = PyPDFLoader(r\"data/DENGUE.pdf\")  \n",
    "    pages_asinc = []  \n",
    "  \n",
    "    # Para leitura assíncrona  \n",
    "    async for page in loader.alazy_load():  \n",
    "        pages_asinc.append(page)  \n",
    "  \n",
    "    # Imprime o resultado  \n",
    "    pretty_print_docs(pages_asinc)  \n",
    "  \n",
    "  \n",
    "# Executa a função assíncrona  \n",
    "await process_pdf()\n",
    "# if __name__ == \"__main__\":  \n",
    "#     await asyncio.gather(process_pdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versão síncrona:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader  \n",
    "  \n",
    "loader = PyPDFLoader(r\"data/DENGUE.pdf\")  \n",
    "pages_sinc = loader.load()  \n",
    "  \n",
    "# Aqui verificaremos que pages_sinc é uma lista de 'Document':  \n",
    "print(\"\\n------ Imprimindo o resultado ------\\n\")  \n",
    "print(pages_sinc)  \n",
    "  \n",
    "print(\"\\n------ Imprimindo o resultado ------\\n\")  \n",
    "\n",
    "pages = []\n",
    "# Acessando os valores carregados:  \n",
    "for elemento in pages_sinc:  \n",
    "    print(\"----- Página Inicio -----\")  \n",
    "    print(f\"Conteúdo: {elemento.page_content}\\n\")  \n",
    "    print(f\"Metadado: {elemento.metadata}\")  \n",
    "    print(\"----- Página Fim    -----\")\n",
    "\n",
    "\n",
    "# async for page in loader.alazy_load():\n",
    "#     pages.append(page)     \n",
    "\n",
    "# print(f\"-> {pages[0].metadata}\\n\")\n",
    "# print(pages[0].page_content)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para PDFs não estruturados, com imagens ou tabelas, é necessário extratores mais complexos e robustos, então convido você a acessar a documentação do LangChain onde há vários exemplos.\n",
    "\n",
    "## Carregador de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Imprimindo o resultado ------\n",
      "\n",
      "Document 1:\n",
      "\n",
      "A 2015 paper, “Neural Machine Translation by Jointly Learning to Align and Translate”,\n",
      "introduced a new approach to addressing this bottleneck. Rather than having\n",
      "the encoder supply a single thought vector, it preserved all the hidden state vectors\n",
      "generated for each token encountered in the encoding process and then allowed\n",
      "the decoder to “soft search” over all of the vectors. As a demonstration, the paper\n",
      "showed that using soft search with an English-to-French translation model increased\n",
      "translation quality significantly. This soft search technique soon came to be known as\n",
      "the attention mechanism.\n",
      "The attention mechanism soon gained a good deal of attention of its own in the\n",
      "AI community, culminating in the 2017 Google Research paper “Attention Is All\n",
      "You Need”, which introduced the transformer architecture shown in Figure 1-4. The\n",
      "transformer retained the high-level structure of its predecessor—consisting of an\n",
      "encoder that received tokens as input followed by a decoder that generated output\n",
      "tokens. But unlike the seq2seq model, all of the recurrent circuitry had been removed,\n",
      "and the transformer instead relies completely upon the attention mechanism. The\n",
      "resulting architecture was very flexible and much better at modeling training data\n",
      "than seq2seq. But whereas seq2seq could process arbitrarily long sequences, the\n",
      "transformer could process only a fixed, finite sequence of inputs and outputs. Since\n",
      "the transformer is the direct progenitor of the GPT models, this is a limitation that we\n",
      "have been pushing back against ever since.\n",
      "\n",
      "GPT Enters the Scene\n",
      "\n",
      "The generative pre-trained transformer architecture was introduced in the 2018\n",
      "paper “Improving Language Understanding by Generative Pre-Training”. The architecture\n",
      "wasn’t particularly special or new. Actually, the architecture was just a transformer\n",
      "with the encoder ripped off—it was just the decoder side. However, this\n",
      "simplification led to some unexpected new possibilities that would only be fully realized\n",
      "in coming years. It was this generative pre-trained transformer architecture—\n",
      "GPT—that would soon ignite the ongoing AI revolution.\n",
      "\n",
      "In 2018, this wasn’t apparent. At that point in time, it was standard practice to pretrain\n",
      "models with unlabeled data—for instance, scraps of text from the internet—and\n",
      "then modify the architecture of the models and apply specialized fine-tuning so that\n",
      "the final model would then be able to do one task very well. And so it was with the\n",
      "generative pre-trained transformer architecture. The 2018 paper simply showed that\n",
      "this pattern worked really well for GPTs—pre-training on unlabeled text followed by\n",
      "supervised fine-tuning for a particular task led to really good models for a variety of\n",
      "tasks such as classification, measuring similarities among documents, and answering\n",
      "multiple-choice questions. But we should emphasize one point: after the GPT was\n",
      "fine-tuned, it was only good at the single task for which it was fine-tuned.\n",
      "GPT-2 was simply a scaled-up version of GPT. When it was introduced in 2019, it\n",
      "was beginning to dawn upon researchers that the GPT architecture was something\n",
      "special. This is clearly evidenced in the second paragraph of the OpenAI blog post\n",
      "introducing GPT-2:\n",
      "\n",
      "Our model, called GPT-2 (a successor to GPT), was trained simply to predict the next\n",
      "word in 40 GB of Internet text. Due to our concerns about malicious applications of\n",
      "the technology, we are not releasing the trained model.\n",
      "Wow! How can those two sentences belong next to each other? How does something\n",
      "as innocuous as predicting the next word—just like an iPhone does when you write a\n",
      "text message—lead to such grave concerns about misuse? If you read the corresponding\n",
      "academic paper, “Language Models Are Unsupervised Multitask Learners”, then\n",
      "you start to find out. GPT-2 was 1.5 billion parameters, as compared with GPT’s 117\n",
      "million, and was trained on 40 GB of text, as compared with GPT’s 4.5 GB. A simple\n",
      "order-of-magnitude increase in model and training set size led to an unprecedented\n",
      "emergent quality—instead of having to fine-tune GPT-2 for a single task, you could\n",
      "apply the raw, pre-trained model to the task and often achieve better results than\n",
      "state-of-the-art models that were fine-tuned specifically for the task.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "As we said at the start, this chapter sets the background for the journey you are about\n",
      "to take into prompt engineering. We started with a discussion of the recent history\n",
      "of language models, and we highlighted why LLMs are so special and different—and\n",
      "why they are fueling the AI revolution that we are all now witnessing. We then\n",
      "defined the topic of this book: prompt engineering.\n",
      "\n",
      "In particular, you should understand that this book isn’t going to be all about how to\n",
      "do nitpicky wording of a single prompt to get one good completion. Sure, we’ll cover\n",
      "that, and we’ll cover in detail all the things you need to do to generate high-quality\n",
      "completions that serve their intended purpose. But when we say, “prompt engineering,”\n",
      "we mean building the entire LLM-based application. The LLM application\n",
      "serves as a transformation layer, iteratively and statefully converting real-world needs\n",
      "into text that LLMs can address and then converting the data provided by the LLMs\n",
      "into information and action that address those real-world needs.\n",
      "\n",
      "Before we set off on this journey, let’s make sure we’re appropriately packed. In the\n",
      "next chapter, you’ll learn how LLM text completion works from the top-level API\n",
      "all the way down to low-level attention mechanisms. In the subsequent chapter, we’ll\n",
      "build upon that knowledge to explain how LLMs have been expanded to handle chat\n",
      "and tool usage, and you’ll see that deep down, it’s really all the same thing—text\n",
      "completion. Then, with those foundational ideas in store, you’ll be ready for your\n",
      "journey.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader  \n",
    "  \n",
    "loader = TextLoader(r\"data/exemplo_arquivo.txt\")  \n",
    "pages_sinc = loader.load()  \n",
    "  \n",
    "# Aqui verificaremos que pages_sinc é uma lista de 'Document':  \n",
    "print(\"\\n------ Imprimindo o resultado ------\\n\")  \n",
    "pretty_print_docs(pages_sinc)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregador de Página Web\n",
    "[link](https://python.langchain.com/docs/how_to/document_loader_web/)\n",
    "\n",
    "Para fazer a leitura de páginas da web no formato LangChain, podemos utilizar a biblioteca `beautifulsoup4` ou `langchain-unstructured`.\n",
    "\n",
    "Importações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-community langchain-unstructured unstructured beautifulsoup4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para uma análise simples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader  \n",
    "  \n",
    "page_url = \"https://python.langchain.com/docs/introduction/\"  \n",
    "  \n",
    "loader = WebBaseLoader(web_paths=[page_url])  \n",
    "resultado = loader.load()  \n",
    "  \n",
    "  \n",
    "# Aqui verificaremos que pages_sinc é uma lista de 'Document':  \n",
    "print(\"\\n------ Imprimindo o resultado ------\\n\")  \n",
    "print(resultado)  \n",
    "  \n",
    "print(\"\\n------ Imprimindo o resultado ------\\n\")  \n",
    "# Acessando os valores carregados:  \n",
    "for elemento in resultado:  \n",
    "    print(\"----- Página Inicio -----\")  \n",
    "    print(f\"Conteúdo: {elemento.page_content}\\n\")  \n",
    "    print(f\"Metadado: {elemento.metadata}\")  \n",
    "    print(\"----- Página Fim    -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para uma análise avançada:\n",
    "\n",
    "Este método é apropriado se quisermos um controle ou processamento mais granular do conteúdo da página. Abaixo, em vez de gerar um `Document`por página e controlar seu conteúdo via BeautifulSoup, geramos vários `Document`objetos representando estruturas distintas em uma página. Essas estruturas podem incluir títulos de seção e seus textos de corpo correspondentes, listas ou enumerações, tabelas e muito mais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader  \n",
    "  \n",
    "page_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"  \n",
    "loader = UnstructuredLoader(web_url=page_url)  \n",
    "  \n",
    "resultado = loader.load()  \n",
    "  \n",
    "  \n",
    "# Aqui verificaremos que pages_sinc é uma lista de 'Document':  \n",
    "print(\"\\n------ Imprimindo o resultado ------\\n\")  \n",
    "print(resultado)  \n",
    "  \n",
    "print(\"\\n------ Imprimindo o resultado ------\\n\")  \n",
    "# Acessando os valores carregados:  \n",
    "for elemento in resultado:  \n",
    "    print(\"----- Página Inicio -----\")  \n",
    "    print(f\"Conteúdo: {elemento.page_content}\\n\")  \n",
    "    print(f\"Metadado: {elemento.metadata}\")  \n",
    "    print(\"----- Página Fim    -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregadores de CSVs\n",
    "[link](https://python.langchain.com/docs/how_to/document_loader_csv/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader  \n",
    "  \n",
    "file_path = r\"exemplo_arquivo.csv\"  \n",
    "  \n",
    "loader = CSVLoader(file_path=file_path)  \n",
    "data = loader.load()  \n",
    "  \n",
    "linha = 0  \n",
    "for record in data:  \n",
    "    print(f\"Imprimindo linha: {linha}\")  \n",
    "    print(f\"-----------\")  \n",
    "    print(record)  \n",
    "    print(f\"-----------\")  \n",
    "    linha+=1  \n",
    "print(\"---- Imprimindo de forma estruturada ----\")  \n",
    "for record in data:  \n",
    "    print(f\"Imprimindo linha: {linha}\")  \n",
    "    print(f\"Conteúdo: {record.page_content}\\n\")  \n",
    "    print(f\"-----------\")  \n",
    "    linha+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregadores de JSON\n",
    "[link](https://python.langchain.com/docs/how_to/document_loader_json/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='exemplo_arquivo.json',\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "for record in data:  \n",
    "    print(f\"Conteúdo: {record.page_content}\\n\")  \n",
    "    print(f\"-----------\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregadores de Markdown\n",
    "[link](https://python.langchain.com/docs/how_to/document_loader_markdown/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "markdown_path = \"Ambiente.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "\n",
    "data = loader.load()\n",
    "assert len(data) == 1\n",
    "assert isinstance(data[0], Document)\n",
    "readme_content = data[0].page_content\n",
    "print(readme_content[:250])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (grassato)",
   "language": "python",
   "name": "grassato"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
