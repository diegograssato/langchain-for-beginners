{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### O que são Embeddings?\n",
    "\n",
    "**Embeddings** são representações vetoriais de textos (ou outros tipos de dados, como imagens, mas aqui focaremos em textos). Quando transformamos um texto em uma lista de números (vetor), podemos realizar diversos cálculos matemáticos para determinar similaridades e relações semânticas entre diferentes pedaços de texto.\n",
    "\n",
    "#### Por que isso é importante?\n",
    "\n",
    "- **Busca semântica (Semantic Search):** Ao comparar vetores de textos, é possível verificar quais são mais “semelhantes” no espaço vetorial. Assim, se tivermos vários documentos e quisermos encontrar aquele que mais se relaciona com uma consulta, basta comparar o embedding da consulta com o embedding de cada documento.\n",
    "- **Clustering (agrupamento):** Podemos agrupar pedaços de texto que falem sobre assuntos semelhantes.\n",
    "- **Classificação e análise de sentimento:** Embora existam métodos específicos para isso, embeddings também podem auxiliar em tarefas de classificação de textos, pois fornecem uma forma numérica de tratar as informações semânticas.\n",
    "\n",
    "No contexto de processamento de linguagem natural (NLP), embeddings são a base para diversos sistemas de recomendação, análise de similaridade, chatbots e muito mais.\n",
    "\n",
    "---\n",
    "\n",
    "#### Como funcionam os Embeddings na LangChain?\n",
    "\n",
    "A classe `Embeddings` da LangChain é uma classe-base para interfaces com vários provedores de modelos de embeddings, como OpenAI, Cohere e Hugging Face. Ela oferece uma padronização, fornecendo dois métodos principais:\n",
    "\n",
    "1. **`.embed_documents`**\n",
    "    \n",
    "    - Recebe como entrada uma lista de textos (strings) e retorna uma **lista de embeddings** (cada embedding corresponde a um dos textos de entrada).\n",
    "    - Exemplo de uso: criar embeddings para um conjunto de documentos em que se deseja fazer busca semântica.\n",
    "2. **`.embed_query`**\n",
    "    \n",
    "    - Recebe um único texto (geralmente uma pergunta/consulta) e retorna um **vetor de floats** (uma única lista de números).\n",
    "    - Exemplo de uso: criar um embedding para a query que será comparada com embeddings de documentos.\n",
    "\n",
    "A diferenciação entre **documentos** e **query** existe porque, em alguns provedores de embeddings, há métodos distintos para processar cada tipo de texto. Além disso, separar as duas funções também deixa claro o fluxo: primeiro criamos embeddings dos documentos (que muitas vezes fica armazenado em algum index ou base de dados), depois criamos embeddings da consulta para comparar e ranquear os documentos mais relevantes.\n",
    "\n",
    "\n",
    "#### Exemplo Prático com OpenAIEmbeddings \n",
    "\n",
    "Aqui, estamos usando o modelo `text-embedding-3-large` da OpenAI, mas existem outros modelos disponíveis (como `text-embedding-ada-002`, por exemplo). Cada modelo pode ter tamanhos diferentes de vetores, impactando a performance e a qualidade das representações.\n",
    "\n",
    "#### Passo 4: Gerar Embeddings de Documentos\n",
    "\n",
    "Suponha que você tenha uma lista de textos que deseja “indexar” ou armazenar para futuras buscas. Podemos usar `.embed_documents` para gerar os vetores correspondentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../helpers/00-llm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.llm import initialize_llm, logger, pretty_print\n",
    " \n",
    "llm, _, embeddings = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = [\n",
    "    \"Olá!\",\n",
    "    \"Quantos anos você tem?\",\n",
    "    \"Qual seu nome?\",\n",
    "    \"Meu amigo se chama flávio\",\n",
    "    \"Oi!\"\n",
    "]\n",
    "\n",
    "embeddings_docs = embeddings.embed_documents(documents)\n",
    "\n",
    "print(embeddings_docs)\n",
    "print(len(embeddings_docs[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo da documentação, o retorno foi algo como `(5, 1536)`, indicando que temos 5 embeddings, cada um com 1536 valores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 5: Gerar Embedding de uma Query\n",
    "\n",
    "Agora, se quisermos pesquisar algo relacionado a “qual é o nome mencionado na conversa?”, podemos criar um embedding específico para essa pergunta e depois comparar com os embeddings dos documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = embeddings.embed_query(\"Qual é o nome do seu amigo?\")\n",
    "print(len(embedded_query))  # Tamanho do vetor da query (ex. 1536)\n",
    "print(embedded_query[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse vetor (`embedded_query`) pode então ser comparado com cada um dos vetores `embeddings` (dos documentos) usando uma métrica de similaridade, como cosseno (cosine similarity). Quanto maior a similaridade, mais relevante é o documento para a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O que Fazer Depois dos Embeddings?\n",
    "\n",
    "1. **Armazenar em um Vetor (Vector Store):**  \n",
    "    Existem diversas bibliotecas de indexação vetorial (ex.: FAISS, Milvus, Pinecone, Qdrant) que permitem armazenar e buscar embeddings de forma eficiente. Você insere cada embedding de documento na base, junto com um identificador do texto. Depois, quando recebe uma query, gera o embedding da query e faz a busca na base para recuperar os documentos mais similares.\n",
    "    \n",
    "2. **Buscar Documentos Relevantes:**  \n",
    "    Após armazenar, podemos buscar documentos usando a similaridade de cosseno (ou outra métrica), retornando os mais próximos do embedding da query.\n",
    "    \n",
    "3. **Utilizar em um sistema RAG:**  \n",
    "    Você pode, por exemplo, criar um chatbot que, com base nos documentos mais relevantes (encontrados por embeddings), gere respostas mais contextualizadas para o usuário.\n",
    "    \n",
    "\n",
    "#### Informação Extra:\n",
    "\n",
    "##### 1. Diferença entre Modelos de Embedding\n",
    "\n",
    "Modelos como `text-embedding-3-large` ou `text-embedding-ada-002` podem variar em termos de:\n",
    "\n",
    "- **Dimensionalidade do vetor** (e.g., 768, 1536 dimensões).\n",
    "- **Qualidade semântica**: alguns modelos capturam relações semânticas mais complexas do que outros.\n",
    "- **Custo e tempo de inferência**: pode haver diferenças de preço por uso (no caso de APIs) ou tempo de computação em cada chamada.\n",
    "\n",
    "##### 2. Melhores Práticas para Performance\n",
    "\n",
    "- **Batching:** Quando for embedar múltiplos documentos, agrupe em lotes para reduzir chamadas sucessivas e otimizar.\n",
    "- **Caching:** Se alguns documentos não mudam com frequência, armazene os embeddings em disco ou em um banco de dados para não precisar recalcular toda vez que iniciar a aplicação.\n",
    "\n",
    "##### Conclusão\n",
    "\n",
    "Os embeddings são peças fundamentais no universo de NLP quando precisamos lidar com similaridade textual, ranking de documentos, chatbots com contexto, entre outros cenários. A LangChain abstrai a lógica de criação de embeddings, permitindo que você troque de provedores (OpenAI, Cohere, Hugging Face) sem grandes refatorações no seu código.\n",
    "\n",
    "Com um simples fluxo de:\n",
    "\n",
    "1. **Gerar embeddings de documentos**\n",
    "2. **Gerar embedding de query**\n",
    "3. **Comparar** usando métricas de similaridade\n",
    "\n",
    "… você já consegue criar sistemas de busca semântica e diversas soluções de linguagem natural de forma mais simples e estruturada.\n",
    "\n",
    "\n",
    "#### Links auxiliares:\n",
    "\n",
    "1 - Preço embeddings: https://openai.com/api/pricing/\n",
    "2 - Modelos abertos no Huggingface: https://huggingface.co/models?other=text-embeddings-inference&sort=trending"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
