{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é um Banco de Dados Vetorial (Vector Store)?\n",
    "\n",
    "Um **banco de dados vetorial** (também chamado de _vector store_) é uma estrutura ou serviço que permite armazenar e pesquisar dados na forma de **vetores de embeddings**. Em aplicações de NLP, esses vetores geralmente são gerados a partir de textos (ou consultas) por meio de um modelo de **embeddings**.\n",
    "\n",
    "#### Por que usar Bancos de Dados Vetoriais?\n",
    "\n",
    "- **Busca Semântica:** Em vez de pesquisar por palavras-chave, você compara vetores (embeddings) e recupera documentos mais relevantes com base na proximidade semântica.\n",
    "- **Escalabilidade:** Muitos _vector stores_ (como Chroma, FAISS, Milvus, Pinecone) são otimizados para lidar com milhões de vetores e executar buscas em tempo satisfatório.\n",
    "- **Operações Avançadas:** Além da busca por similaridade, alguns bancos suportam filtros por metadados, reordenação de resultados para diversidade (Maximal Marginal Relevance), pesquisa híbrida (keyword + similaridade), etc.\n",
    "\n",
    "Quando utilizamos um banco de dados vetorial, o **fluxo** geral é:\n",
    "\n",
    "1. **Carregar e dividir textos** (opcional, mas comum em caso de documentos extensos).\n",
    "2. **Gerar embeddings** para cada pedaço de texto.\n",
    "3. **Armazenar** esses embeddings (e seus metadados) no _vector store_.\n",
    "4. **Consultar** o _vector store_ a partir de um texto ou de um embedding para recuperar os documentos mais relevantes de maneira semântica.\n",
    "\n",
    "---\n",
    "\n",
    "##### Exemplo Prático de Criação e Consulta de um Banco de Dados Vetorial (Chroma db)\n",
    "\n",
    "[Documentação](https://python.langchain.com/docs/integrations/vectorstores/chroma/)\n",
    "\n",
    "Considere o texto:\n",
    " \n",
    "\n",
    "Vamos criar um documento dele como se fosse feito obtido via `document loaders`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../helpers/00-llm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"A inteligência artificial (IA) é uma área da ciência da computação que tem revolucionado diversas indústrias e aspectos da vida cotidiana. Mas, o que realmente significa \"inteligência artificial\"? Trata-se de sistemas computacionais capazes de realizar tarefas que, anteriormente, só poderiam ser executadas por seres humanos, como reconhecimento de fala, tomada de decisão e aprendizado com dados. Impressionante, não é? Esses sistemas utilizam algoritmos avançados e grandes volumes de dados para identificar padrões, adaptarem-se a novas situações e fornecerem soluções inovadoras.\n",
    "\n",
    "Um dos maiores avanços recentes em IA é o aprendizado profundo (ou deep learning). Essa técnica permite que máquinas realizem tarefas extremamente complexas, como diagnosticar doenças a partir de imagens médicas ou até mesmo compor músicas! Curioso como isso funciona? Redes neurais artificiais – inspiradas no funcionamento do cérebro humano – processam informações em múltiplas camadas, identificando nuances que seriam impossíveis para métodos tradicionais. Como resultado, a IA tem transformado áreas como saúde, finanças e transporte, promovendo eficiência e inovação em escala global.\n",
    "\n",
    "No entanto, a expansão da inteligência artificial também levanta questões importantes. Estamos preparados para lidar com os desafios éticos que a IA traz? Por exemplo: como garantir que algoritmos de IA sejam imparciais e inclusivos? Além disso, há preocupações sobre o impacto no mercado de trabalho – algumas profissões podem ser substituídas por máquinas. Apesar desses desafios, uma coisa é certa: a inteligência artificial já não é mais uma tecnologia do futuro; é uma realidade do presente, moldando o mundo ao nosso redor com potencial ilimitado!\"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "texto_original = Document(page_content=text)\n",
    "\n",
    "# Vamos criar agora uma lista de documentos:\n",
    "docs = [texto_original]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 1: Preparar o Ambiente\n",
    "\n",
    "**Instalar ou configurar as dependências** que serão utilizadas. Por exemplo, se formos usar o banco vetorial **Chroma** (local) e o modelo de embeddings da OpenAI, podemos fazer:\n",
    "\n",
    "Caso use outro _vector store_, como FAISS, Pinecone, Milvus, etc., instale a biblioteca correspondente.\n",
    "\n",
    "#### Passo 2: Inicializar o Modelo de Embeddings\n",
    "\n",
    "Como todo _vector store_ depende de vetores de embeddings, vamos configurar a **chave de API** da OpenAI (caso ainda não esteja definida) e criar o nosso objeto de embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.llm import initialize_llm, logger, pretty_print\n",
    " \n",
    "llm, _, embeddings = initialize_llm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 3: Carregar e (Opcionalmente) Dividir Documentos\n",
    "\n",
    "Para exemplificar, vamos carregar um arquivo de texto (`state_of_the_union.txt`). Em seguida, dividiremos esse texto em “chunks” menores, pois para a busca semântica normalmente trabalhamos com pedaços de texto menores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "# Divide o documento em partes menores (chunks) de tamanho 1000 caracteres\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documentos = text_splitter.split_documents(docs)\n",
    "\n",
    "print(documentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 4: Criar o Banco Vetorial (Exemplo: Chroma)\n",
    "\n",
    "Agora, utilizamos o `Chroma` (um _vector store_ que roda localmente) para armazenar os embeddings dos documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Cria o banco de dados vetorial, gerando os embeddings dos documentos\n",
    "Chroma.from_documents(documentos, collection_name=\"nome_colecao\", embedding=embeddings, persist_directory=\"./meu_banco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sob o capô, cada chunk de texto de `documents` é convertido em um vetor de floats (pelo modelo de embeddings) e armazenado no banco, junto com alguma forma de identificação e metadados.\n",
    "\n",
    "#### Passo 5: Realizar Buscas Semânticas\n",
    "\n",
    "Depois de criado, podemos consultar o _vector store_ usando, por exemplo, o método `.similarity_search(query)` para buscar documentos que respondam a uma pergunta ou se relacionem a um tópico específico.\n",
    "\n",
    "Logo, primeiro conectar ao banco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(  \n",
    "    persist_directory=\"./meu_banco\",  \n",
    "    collection_name=\"nome_colecao\",  \n",
    "    embedding_function=embeddings,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Na expansão da inteligência artificial quais questões importantes são levantadas?\"\n",
    "pedacoes_retornados = db.similarity_search(query, k=2)\n",
    "\n",
    "# Total de docs retornados -> O default de similarity_search é K=3, mas escolhemos k= 2\n",
    "print(len(pedacoes_retornados))\n",
    "# Exibir o conteúdo do primeiro documento retornado\n",
    "print(pedacoes_retornados[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo interno:\n",
    "\n",
    "1. Gera embedding para a `query`.\n",
    "2. Compara esse embedding com cada um dos embeddings armazenados.\n",
    "3. Retorna os documentos mais semanticamente similares.\n",
    "\n",
    "---\n",
    "\n",
    "#### Interface\n",
    "\n",
    "Para facilitar o uso e padronizar a interação, a LangChain oferece uma **interface de vector store** que define métodos básicos:\n",
    "\n",
    "- **`add_documents(documents)`:**  \n",
    "    Adiciona documentos (lista de `Document`) ao banco vetorial.\n",
    "    \n",
    "    - `documents`: lista de objetos `Document`, cada um contendo o texto (`page_content`) e metadados.\n",
    "    - `ids`: lista opcional com identificadores únicos para cada documento.\n",
    "- **`delete_documents(ids=[...])`:**  \n",
    "    Remove documentos específicos, identificados por seus IDs.\n",
    "    \n",
    "- **`similarity_search(query, k=4, filter=None)`:**  \n",
    "    Faz a busca semântica, retornando os `k` documentos mais similares ao `query`.\n",
    "    \n",
    "    - `query`: texto que será transformado em embedding.\n",
    "    - `k`: quantidade de documentos mais similares a retornar.\n",
    "    - `filter`: dicionário ou estrutura de filtragem baseado em metadados, se suportado pelo vector store.\n",
    "- **`similarity_search_by_vector(embedding_vector, k=4, filter=None)`:**  \n",
    "    Variante de busca que recebe diretamente um vetor de embedding ao invés de uma string.\n",
    "    \n",
    "\n",
    "Cada implementação de vector store (Chroma, FAISS, Pinecone, Qdrant etc.) segue a mesma estrutura de interface, porém pode oferecer recursos adicionais como re-indexação, reordenação avançada, busca híbrida, entre outros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outras Funcionalidades Importantes\n",
    "\n",
    "##### 1. Similaridade por Vetor Direto\n",
    "\n",
    "Em vez de passar uma string, podemos gerar manualmente o embedding e chamar `.similarity_search_by_vector(embedding_vector)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = embeddings.embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso é útil se você já tiver o embedding calculado ou se quiser maior flexibilidade de manipular os vetores antes de pesquisar.\n",
    "\n",
    "##### 2. Operações Assíncronas\n",
    "\n",
    "Em cenários de alta carga ou quando seu _vector store_ está hospedado em um serviço externo, chamadas assíncronas podem trazer ganho de performance. A LangChain fornece métodos com `a` de assíncrono, como `asimilarity_search`. Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = await db.asimilarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso permite que seu código não fique bloqueado enquanto aguarda a resposta do serviço.\n",
    "\n",
    "##### 3. Exclusão e Atualização de Documentos\n",
    "\n",
    "Alguns _vector stores_ permitem remover ou atualizar documentos. Na interface padrão, temos métodos como `.delete_documents(ids=[])`, para remover documentos com base em seus IDs. Isso é útil quando um documento precisa ser retirado do seu índice, ou se deseja fazer uma substituição de conteúdo.\n",
    "\n",
    "##### 4. Parâmetros Avançados de Busca\n",
    "\n",
    "- **k**: Quantidade de documentos mais similares a retornar.\n",
    "- **filter**: Possibilidade de filtrar por metadados. Por exemplo, retornar apenas documentos cujo campo `source` tenha valor `\"tweet\"`.\n",
    "\n",
    "Exemplo de uso (Pinecone ou outros que suportem filtros de metadados):\n",
    "\n",
    "Imagine que no banco há vários tweets diferentes e postagens do threads. Para filtrar da coleção apenas os dados de tweet, poderíamos usar o parâmetro de filtro: `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine que no banco há vários tweets diferentes e postagens do threads. Para filtrar da coleção apenas os dados de tweet, poderíamos usar o parâmetro de filtro: `filter`.\n",
    "\n",
    "docs = db.similarity_search(\n",
    "    query=\"Quais tweet \",\n",
    "    k=2,\n",
    "    filter={\"source\": \"tweet\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas opções variam conforme o _vector store_, então verifique a documentação específica.\n",
    "\n",
    "---\n",
    "\n",
    "### Conceitos Fundamentais\n",
    "\n",
    "##### Métricas de Similaridade\n",
    "\n",
    "Para comparar vetores, são utilizadas funções matemáticas que avaliam a “distância” ou “semelhança” entre dois vetores:\n",
    "\n",
    "- **Similaridade de Cosseno (Cosine Similarity):** Mede o cosseno do ângulo entre dois vetores.\n",
    "- **Distância Euclidiana:** Mede a distância em linha reta entre dois pontos/vetores.\n",
    "- **Dot Product (Produto Interno):** Mede a projeção de um vetor em outro.\n",
    "\n",
    "A decisão sobre qual métrica usar depende do modelo de embeddings e do comportamento desejado. Frequentemente, “cosine similarity” é usada para lidar com texto.\n",
    "\n",
    "##### Algoritmos de Indexação\n",
    "\n",
    "Para evitar comparar exaustivamente o embedding de cada documento com o embedding da query, a maior parte dos _vector stores_ adota estruturas de indexação como **HNSW** ou **IVF** (no caso do FAISS), que permitem buscas aproximadas e muito mais rápidas em grandes conjuntos de vetores.\n",
    "\n",
    "---\n",
    "\n",
    "##### Metadados e Filtros\n",
    "\n",
    "Quando inserimos documentos, podemos armazenar junto metadados como autor, data, fonte, etc. Na hora da busca, alguns bancos vetoriais oferecem filtros para recortar apenas documentos que batam com determinado critério de metadado. Isso possibilita **buscar por contexto** (por exemplo, “só me mostre documentos da categoria X ou do autor Y”).\n",
    "\n",
    "---\n",
    "\n",
    "##### Técnicas Avançadas\n",
    "\n",
    "1. **Hybrid Search (Busca Híbrida):** Combina busca por similaridade semântica (via embeddings) com busca por palavra-chave (keyword). Ajuda a conciliar os dois mundos: correspondências exatas (palavras-chave) e correspondência semântica.\n",
    "    \n",
    "2. **Maximal Marginal Relevance (MMR):** Serve para reordenar resultados para aumentar a diversidade das respostas, evitando que todos os resultados sejam muito parecidos entre si.\n",
    "    \n",
    "3. **Re-rankers e Transformers:** Em cenários mais complexos, pode-se aplicar uma segunda etapa de reordenação, usando um modelo transformer para verificar a relevância de cada candidato.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "##### Conclusão\n",
    "\n",
    "Bancos de dados vetoriais (**vector stores**) são parte essencial de aplicações avançadas de NLP e IA generativa. Eles permitem armazenar e recuperar documentos com base em **similaridade semântica**, o que torna as buscas muito mais poderosas do que buscas baseadas em palavras-chave.\n",
    "\n",
    "**Passo a passo resumido:**\n",
    "\n",
    "1. **Carregar os documentos** e, opcionalmente, dividi-los em chunks menores.\n",
    "2. **Gerar embeddings** para cada chunk com um modelo, como o da OpenAI.\n",
    "3. **Armazenar** esses vetores e metadados em um _vector store_ (Chroma, FAISS, Pinecone etc.).\n",
    "4. **Consultar** esses dados via `.similarity_search` (inserindo texto) ou `.similarity_search_by_vector` (inserindo vetores).\n",
    "5. **Filtrar ou refinar** resultados usando técnicas avançadas como metadados, MMR, busca híbrida, etc.\n",
    "\n",
    "Com essa base, você já está preparado para criar e consultar bancos de dados vetoriais para múltiplas aplicações: motores de busca semântica, chatbots contextuais, recomendação de conteúdo, análise de similaridade, e muito mais!\n",
    "\n",
    "##### Links Auxiliares:\n",
    "\n",
    "1 - Bancos vetoriais suportados pelo LangChain: https://python.langchain.com/docs/integrations/vectorstores/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "from langchain_community.document_loaders import TextLoader  \n",
    " \n",
    "from langchain_text_splitters import CharacterTextSplitter  \n",
    "from langchain_chroma import Chroma  \n",
    "  \n",
    "  \n",
    "# Função: Divide o documento em partes menores (chunks) de tamanho 1000 caracteres, com prioriade para a quebra por  \n",
    "# paragrafo.  \n",
    "def divide_texto(lista_documento_entrada):  \n",
    "    print(f\">>> REALIZANDO A DIVISAO DO TEXXTO ORIGINAL EM CHUNKS\")  \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)  \n",
    "    documents = text_splitter.split_documents(lista_documento_entrada)  # usado split_documents dado que a entrada é uma lista de documentos:  \n",
    "    i = 0  \n",
    "    for pedaco in documents:  \n",
    "        print(\"--\" * 30)  \n",
    "        print(f\"Chunk: {i}\")  \n",
    "        print(\"--\" * 30)  \n",
    "        print(pedaco)  \n",
    "        print(\"--\" * 30)  \n",
    "        i += 1  \n",
    "    return documents  \n",
    "  \n",
    "  \n",
    "# Cria o banco de dados vetorial, gerando os embeddings dos documentos  \n",
    "def cria_banco_vetorial_e_indexa_documentos(documentos):  \n",
    "    print(f\">>> REALIZANDO INDEXAÇÃO DOS CHUNKS NO BANCO VETORIAL\")  \n",
    "    # Cria o banco de dados vetorial, gerando os embeddings dos documentos  \n",
    "    # Adicionar os chunks no banco em lote    \n",
    "    Chroma.from_documents(documentos, collection_name=\"nome_colecao\", embedding=embeddings, persist_directory=\"./meu_banco\")  \n",
    "  \n",
    "  \n",
    "  \n",
    "def ler_txt_e_retorna_texto_em_document():  \n",
    "    print(f\">>> REALIZANDO A LEITURA DO TXT EXEMPLO\")  \n",
    "    # lendo o txt com o texto exemplo e criando o Document:  \n",
    "    lista_documentos = TextLoader('data/exemplo_texto.txt', encoding='utf-8').load()  \n",
    "  \n",
    "    print(\"Texto lido e convertido em Document\")  \n",
    "    print(lista_documentos)  \n",
    "    print(\"-----------------------------------\")  \n",
    "    return lista_documentos  \n",
    "  \n",
    "  \n",
    "def conecta_banco_vetorial_pre_criado():  \n",
    "    vector_store_from_client = Chroma(  \n",
    "        persist_directory=\"./meu_banco\",  \n",
    "        collection_name=\"nome_colecao\",  \n",
    "        embedding_function=embeddings,  \n",
    "    )  \n",
    "    return vector_store_from_client  \n",
    "  \n",
    "  \n",
    "# Verifica se o diretório \"./meu_banco\" não existe  \n",
    "if not os.path.exists(\"./meu_banco\"):  \n",
    "    print(\"O diretório './meu_banco' não existe... realizando a indexação\")  \n",
    "    texto_completo_lido = ler_txt_e_retorna_texto_em_document()  \n",
    "    divide_texto = divide_texto(texto_completo_lido)  \n",
    "    cria_banco_vetorial_e_indexa_documentos(divide_texto)  \n",
    "else:  \n",
    "    print(\"O diretório './meu_banco' já existe. Pulando a criação do banco vetorial.\")  \n",
    "  \n",
    "# Conectando ao banco vetorial pre criado com os dados indexados:  \n",
    "db = conecta_banco_vetorial_pre_criado()  \n",
    "  \n",
    "# Agora podemos trabalhar com o banco uma vez que ele está com os dados já indexados.  \n",
    "  \n",
    "query = \"Na expansão da inteligência artificial quais questões importantes são levantadas?\"  \n",
    "pedacoes_retornados = db.similarity_search(query, k=2)  \n",
    "  \n",
    "  \n",
    "# Total de docs retornados  \n",
    "print(\"Total de pedaços. Deve ter o valor de 'K':\")  \n",
    "print(len(pedacoes_retornados))  \n",
    "# Exibir o conteúdo do primeiro documento retornado  \n",
    "# Imprimindo os pedaços retornados do banco:  \n",
    "i=0  \n",
    "for elm in pedacoes_retornados:  \n",
    "    print(f\"------ chunk {i} -------\")  \n",
    "    print(pedacoes_retornados[i].page_content)  \n",
    "    print(\"--------------------\")  \n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
