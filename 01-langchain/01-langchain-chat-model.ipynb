{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Componentes Fundamentais:\n",
    "\n",
    "- **Models (Modelos):** LangChain serve como uma interface padrão que permite interações com uma ampla gama de Grandes Modelos de Linguagem (LLMs).\n",
    "\n",
    "- **Chains (Cadeias):** Como seu nome implica, _cadeias_ são o núcleo dos fluxos de trabalho do LangChain. Combinam LLMs com outros componentes, criando aplicativos por meio da execução de uma sequência de funções.\n",
    "\n",
    "- **Prompts (Instruções):** Os prompts são as instruções apresentadas a um LLM. Geralmente, a \"arte\" de redigir prompts que efetivamente entregam o contexto necessário para que o LLM interprete a entrada e a saída da estrutura da maneira mais útil para você é chamada de engenharia de prompt.\n",
    "\n",
    "- **Indexes (Índices):** Para realizar determinadas tarefas, as LLMs precisarão acessar fontes de dados externas específicas não incluídas em seu conjunto de dados de treinamento, como documentos internos, e-mails ou conjuntos de dados. LangChain refere-se coletivamente a essa documentação externa como “índices\".\"\n",
    "\n",
    "- **Memory (Memória):** Por padrão, os LLMs não têm memória de longo prazo de conversas anteriores (a menos que o histórico do chat seja usado como entrada para uma consulta). O LangChain soluciona esse problema com utilitários simples para adicionar memória a um sistema, com opções que vão desde a retenção total de todas as conversas até a retenção de um resumo da conversa até a retenção das _n_ trocas mais recentes.\n",
    "\n",
    "- **Agents/Tools (Agentes/Ferramentas):** Os agentes do LangChain podem usar um determinado modelo LLM como um \"mecanismo de raciocínio\" para determinar quais ações tomar. Ao criar uma cadeia para um agente, as entradas contêm:\n",
    "\n",
    "\t- uma lista de ferramentas disponíveis para serem aproveitadas.\n",
    "\t- entrada do usuário (como prompts e consultas).\n",
    "\t- quaisquer etapas relevantes executadas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../helpers/00-llm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "\n",
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = client.responses.create(\n",
    "    model=model,\n",
    "    input=\"Fale sobre SOLID\",\n",
    "    store=True,\n",
    ")\n",
    "\n",
    "print(response.output_text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "response = client.completions.create(\n",
    "  model=model,\n",
    "  prompt=\"Crie uma canção contendo apenas uma estrofe.\"\n",
    ")\n",
    "\n",
    "message = (response.choices[0].text)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Conte uma piada sobre programação\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)\n",
    "message = (response.choices[0].message.content)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Você é um assistente de investimentos fictício.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Qual é o melhor investimento de baixo risco que você recomenda para este ano?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "message = (response.choices[0].message.content)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiper Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency_penalty - justa a probabilidade de repetição de frase/palavras, reduzindo a redundância(positivo) ou aumentando a repetição(negativo)-2 e 2, padrão é 0\n",
    "# presence_penalty - penaliza a inclusão de palavras novas, incentivando a diversidade\n",
    "# max_tokens - define o número máximo de tokens na resposta, limitando o tamanho da resposta\n",
    "# temperature - controla a aleatoriedade das respostas, mais baixo (0.0) gera respostas mais previsíveis, enquanto valores mais altos (até 1.0) geram respostas mais criativas e variadas\n",
    "# n - número de respostas a serem geradas, padrão é 1\n",
    "# seed -  valor de inicialização da API, aumenta a probabilidade de você obter a mesma resposta para a mesma pergunta, útil para testes e depuração\n",
    "# stop - ele vai fazer com que o modelo pare de gerar texto quando encontrar uma sequência específica, útil para controlar o final da resposta\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  frequency_penalty=1,\n",
    "  presence_penalty = 1,\n",
    "  temperature =  1 ,\n",
    "  max_tokens=500,\n",
    "  n = 2 ,\n",
    "  seed = 123,\n",
    "  #stop = [\"shadows\",\"mortal \"],\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Você é um poeta deprimido e desiludido.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Componha um poema no máximo 5 linhas e 3 parágrafos.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(\" -------------------------\")\n",
    "print(response.choices[1].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O ChatModel é um componente LangChain então ele possui o protocolo invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from helpers.llm import initialize_llm, logger, pretty_print\n",
    "\n",
    "llm, _, _ = initialize_llm()\n",
    "\n",
    "logger.info(\"LLM and embeddings initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta = llm.invoke(\"Olá como você está e o que você é capaz de fazer?\")\n",
    "     \n",
    "pretty_print(resposta)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando a conversa. \n",
    "\n",
    "Lembrando que os ChatModels recebem como entrada uma lista de mensagem. Assim o LangChain automaticamente converte isso na estrutura que o modelo LLM precisa receber para responder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Forma 1 de escrever:\n",
    "mensagens = [\n",
    "\t\t\t SystemMessage(content=\"Você é um especialista em astrofísica.\"),\n",
    "\t\t\t HumanMessage(content=\"Qual a distancia do sol até a terra?\"),\n",
    "\t\t\t AIMessage(content=\"O Sol está a 49.600.000 km de distância da Terra.\"),\n",
    "\t\t\t HumanMessage(content=\"E a distância da terra até marte?\"),\n",
    "]\n",
    "\n",
    "# Forma 2 de escrever:\n",
    "# mensagens = [\n",
    "# \t\t\t (\"system\", \"Você é um especialista em astrofísica.\"),\n",
    "# \t\t\t (\"user\", \"Qual a distancia do sol até a terra?\"),\n",
    "# \t\t\t (\"assistant\", \"O Sol está a&nbsp;49.600.000 km de distância da Terra.\"),\n",
    "#            (\"user\", \"E a distância da terra até marte?\"),\n",
    "# ]\n",
    "\n",
    "# Como a entrada do usuário é a ultima mensagem da lista, você pode dá invoke usando a lista de pensagens contendo o histórico de conversação.\n",
    "resposta = llm.invoke(mensagens)\n",
    "\n",
    "pretty_print(resposta)\n",
    " \n",
    "\n",
    "print(\"-------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora criar um chat, ou seja, vamos criar uma lista que vai crescendo dinamicamente com a entrada do usuário simulando uma conversa com ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora simular o streaming de dados dos modelos (quando compatível) onde cada token é gerado em tempo de execução. Vamos repetir o código anterior e ao invés de invoke, vamos chamar o modelo usando a função assíncrona astream:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rodando modelo localmente\n",
    "\n",
    "Vamos agora fazer um teste rodando o ollama local utilizando um container docker\n",
    "\n",
    "\n",
    "```bash\n",
    "\n",
    "docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "docker exec -it ollama bash\n",
    "ollama pull llama3.2\n",
    "\n",
    " \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"prompt\":\"Quem é Linux Torvald?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl http://localhost:11434/api/chat -d '{ \"model\": \"llama3.2\", \"messages\": [ { \"role\": \"user\", \"content\": \"Por que o céu é azul?\" }]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "OllamaLLM(  \n",
    "     base_url=\"http://localhost:11434\",  \n",
    "     model = \"llama3.2\",  # nome do modelo que deseja tem em sua máquina\n",
    "     temperature = 0.3,  \n",
    "     num_predict = 1000, # numero máximo de tokens\n",
    " )\n",
    "\n",
    "template = \"\"\"\n",
    "Você é um especialista em música brasileira e tem amplo conhecimento sobre letras de músicas.\n",
    "\n",
    "Trecho da música: {question}\n",
    "\n",
    "Resposta: Dê continuidade a música e o final conte seu histórico.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model = \"llama3.2\") \n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"question\": \"Ouviram do Ipiranga às margens plácidas\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a versão via chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "chat = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "response = chat.invoke(\"Conte uma piada\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Faça perguntas sobre a arquitetura de software, você ajudará o usuário com perguntas da area, e responderá sempre com o minimo de informações.\"),\n",
    "    HumanMessage(content=\"O que é SOLID?\")\n",
    "]\n",
    " \n",
    "response_stream = chat.stream(messages)\n",
    "for stream in response_stream:\n",
    "    print(stream.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos apagar o container para não ficar ocupando espaço\n",
    "\n",
    "\n",
    "```bash\n",
    "docker container stop ollama\n",
    "docker container rm ollama\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain)",
   "language": "python",
   "name": "grassato"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
