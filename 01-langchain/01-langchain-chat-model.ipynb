{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Componentes Fundamentais:\n",
    "\n",
    "- **Models (Modelos):** LangChain serve como uma interface padrão que permite interações com uma ampla gama de Grandes Modelos de Linguagem (LLMs).\n",
    "\n",
    "- **Chains (Cadeias):** Como seu nome implica, _cadeias_ são o núcleo dos fluxos de trabalho do LangChain. Combinam LLMs com outros componentes, criando aplicativos por meio da execução de uma sequência de funções.\n",
    "\n",
    "- **Prompts (Instruções):** Os prompts são as instruções apresentadas a um LLM. Geralmente, a \"arte\" de redigir prompts que efetivamente entregam o contexto necessário para que o LLM interprete a entrada e a saída da estrutura da maneira mais útil para você é chamada de engenharia de prompt.\n",
    "\n",
    "- **Indexes (Índices):** Para realizar determinadas tarefas, as LLMs precisarão acessar fontes de dados externas específicas não incluídas em seu conjunto de dados de treinamento, como documentos internos, e-mails ou conjuntos de dados. LangChain refere-se coletivamente a essa documentação externa como “índices\".\"\n",
    "\n",
    "- **Memory (Memória):** Por padrão, os LLMs não têm memória de longo prazo de conversas anteriores (a menos que o histórico do chat seja usado como entrada para uma consulta). O LangChain soluciona esse problema com utilitários simples para adicionar memória a um sistema, com opções que vão desde a retenção total de todas as conversas até a retenção de um resumo da conversa até a retenção das _n_ trocas mais recentes.\n",
    "\n",
    "- **Agents/Tools (Agentes/Ferramentas):** Os agentes do LangChain podem usar um determinado modelo LLM como um \"mecanismo de raciocínio\" para determinar quais ações tomar. Ao criar uma cadeia para um agente, as entradas contêm:\n",
    "\n",
    "\t- uma lista de ferramentas disponíveis para serem aproveitadas.\n",
    "\t- entrada do usuário (como prompts e consultas).\n",
    "\t- quaisquer etapas relevantes executadas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../helpers/00-llm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from helpers.llm import initialize_llm, logger, pretty_print\n",
    "\n",
    "llm, _, _ = initialize_llm()\n",
    "\n",
    "logger.info(\"LLM and embeddings initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### O ChatModel é um componente LangChain então ele possui o protocolo invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta = llm.invoke(\"Olá como você está e o que você é capaz de fazer?\")\n",
    "     \n",
    "pretty_print(resposta)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando a conversa. \n",
    "\n",
    "Lembrando que os ChatModels recebem como entrada uma lista de mensagem. Assim o LangChain automaticamente converte isso na estrutura que o modelo LLM precisa receber para responder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Forma 1 de escrever:\n",
    "mensagens = [\n",
    "\t\t\t SystemMessage(content=\"Você é um especialista em astrofísica.\"),\n",
    "\t\t\t HumanMessage(content=\"Qual a distancia do sol até a terra?\"),\n",
    "\t\t\t AIMessage(content=\"O Sol está a 49.600.000 km de distância da Terra.\"),\n",
    "\t\t\t HumanMessage(content=\"E a distância da terra até marte?\"),\n",
    "]\n",
    "\n",
    "# Forma 2 de escrever:\n",
    "# mensagens = [\n",
    "# \t\t\t (\"system\", \"Você é um especialista em astrofísica.\"),\n",
    "# \t\t\t (\"user\", \"Qual a distancia do sol até a terra?\"),\n",
    "# \t\t\t (\"assistant\", \"O Sol está a&nbsp;49.600.000 km de distância da Terra.\"),\n",
    "#            (\"user\", \"E a distância da terra até marte?\"),\n",
    "# ]\n",
    "\n",
    "# Como a entrada do usuário é a ultima mensagem da lista, você pode dá invoke usando a lista de pensagens contendo o histórico de conversação.\n",
    "resposta = llm.invoke(mensagens)\n",
    "\n",
    "pretty_print(resposta)\n",
    " \n",
    "\n",
    "print(\"-------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora criar um chat, ou seja, vamos criar uma lista que vai crescendo dinamicamente com a entrada do usuário simulando uma conversa com ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora simular o streaming de dados dos modelos (quando compatível) onde cada token é gerado em tempo de execução. Vamos repetir o código anterior e ao invés de invoke, vamos chamar o modelo usando a função assíncrona astream:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (grassato)",
   "language": "python",
   "name": "grassato"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
