{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é LangChain\n",
    "\n",
    "- Desenvolvido por Harrison Chase.\n",
    "- Lançado em 2022\n",
    "\n",
    "**LangChain** é uma estrutura (framework) open source desenvolvida para facilitar o desenvolvimento de aplicativos e pipelines que integram modelos de linguagem de grande porte (LLMs) com outras fontes de dados e componentes de software. \n",
    "\n",
    "O objetivo principal do LangChain é ajudar desenvolvedores a criar aplicações complexas que podem usar modelos de linguagem de maneira eficiente e interagir com fontes de dados externas, como bancos de dados, APIs, e outros tipos de dados dinâmicos.\n",
    "\n",
    "O LangChain permite ao desenvolvedor criar um ambiente centralizado para construir aplicações LLMs e integrá-los ao seu sistema. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Componentes Fundamentais:\n",
    "\n",
    "- **Models (Modelos):** LangChain serve como uma interface padrão que permite interações com uma ampla gama de Grandes Modelos de Linguagem (LLMs).\n",
    "\n",
    "- **Chains (Cadeias):** Como seu nome implica, _cadeias_ são o núcleo dos fluxos de trabalho do LangChain. Combinam LLMs com outros componentes, criando aplicativos por meio da execução de uma sequência de funções.\n",
    "\n",
    "- **Prompts (Instruções):** Os prompts são as instruções apresentadas a um LLM. Geralmente, a \"arte\" de redigir prompts que efetivamente entregam o contexto necessário para que o LLM interprete a entrada e a saída da estrutura da maneira mais útil para você é chamada de engenharia de prompt.\n",
    "\n",
    "- **Indexes (Índices):** Para realizar determinadas tarefas, as LLMs precisarão acessar fontes de dados externas específicas não incluídas em seu conjunto de dados de treinamento, como documentos internos, e-mails ou conjuntos de dados. LangChain refere-se coletivamente a essa documentação externa como “índices\".\"\n",
    "\n",
    "- **Memory (Memória):** Por padrão, os LLMs não têm memória de longo prazo de conversas anteriores (a menos que o histórico do chat seja usado como entrada para uma consulta). O LangChain soluciona esse problema com utilitários simples para adicionar memória a um sistema, com opções que vão desde a retenção total de todas as conversas até a retenção de um resumo da conversa até a retenção das _n_ trocas mais recentes.\n",
    "\n",
    "- **Agents/Tools (Agentes/Ferramentas):** Os agentes do LangChain podem usar um determinado modelo LLM como um \"mecanismo de raciocínio\" para determinar quais ações tomar. Ao criar uma cadeia para um agente, as entradas contêm:\n",
    "\n",
    "\t- uma lista de ferramentas disponíveis para serem aproveitadas.\n",
    "\t- entrada do usuário (como prompts e consultas).\n",
    "\t- quaisquer etapas relevantes executadas anteriormente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Engineering\n",
    "Engenharia de prompts é o processo de projetar e otimizar prompts para tarefas de processamento de linguagem natural. Envolve selecionar os prompts corretos, ajustar seus parâmetros e avaliar seu desempenho. A engenharia de prompts é crucial para alcançar alta precisão e eficiência em modelos de PLN.\n",
    "\n",
    "\n",
    "\n",
    "#### LCEL (LangChain Expression Language)\n",
    "\n",
    "Como falamos anteriormente, os Runnables são os unidades básicas de trabalho do LangChain, ou seja, é um protocolo que implementa as interfaces de `invoke`, `stream` e `batch` bem como suas variantes assíncronas. \n",
    "\n",
    "Além disso, o LangChain implementa a forma declarativa de compor cadeias utilizando o operador pipe \"|\".\n",
    "\n",
    "Vantagens de utilizar a forma declarativa:\n",
    "\n",
    "- **Suporte de streaming de primeira classe**: Quando você constrói suas cadeias com LCEL, você obtém o melhor time-to-first-token possível (tempo decorrido até que o primeiro pedaço de saída saia).\n",
    "- **Suporte assíncrono**: Qualquer cadeia construída com LCEL pode ser chamada tanto com a API síncrona quanto com a API assíncrona. Isso permite usar o mesmo código para protótipos e em produção, com ótimo desempenho e a capacidade de lidar com muitas solicitações simultâneas no mesmo servidor.\n",
    "- **Execução paralela otimizada**.\n",
    "- **Novas tentativas e fallbacks**: você consegue configurar novas tentativas e fallbacks para qualquer parte da sua cadeia LCEL.\n",
    "- **Acessar resultados intermediários**: Para cadeias mais complexas, geralmente é muito útil acessar os resultados de etapas intermediárias antes mesmo que a saída final seja produzida.\n",
    "- **Esquemas de entrada e saída**: Os esquemas de entrada e saída fornecem a cada cadeia LCEL esquemas Pydantic e JSONSchema inferidos da estrutura da sua cadeia.\n",
    "\n",
    "Cada componente é conectado usando o operador pipe \"|\" (ou usando `.pipe()`) , ou seja, ele cria a arquitetura de pipeline, onde a saída de uma função (ou componente) é tratado como entrada de proxima função (ou, componente), permitindo criar uma cadeia (chain) sequencial de ações.\n",
    "\n",
    "Grande parte dos componentes do LangChain segue o protocolo `Runnable` (executáveis) que automaticamente implementa as interfaces de `invoke`, `stream` e `batch` bem como suas variantes assincronas.\n",
    "\n",
    "O `Runnables` é a unidade de trabalhado do LangChain ou seja, uma vez que o componente é criado usando como base os '`Runnables`' , este componente adquire a capacidade de  ser invocado, agrupado, transmitido, transformado e composto. \n",
    "\n",
    "Vantagem de usar a linguagem LCEL (LangChain Expression Language): maneira declarativa de encadear componentes do LangChain:\n",
    "- Código Limpo\n",
    "- Fácil Manutenção\n",
    "- Simplicidade    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo 1: Criando o prompt simples\n",
    "\n",
    "Podem reparar que o texto abaixo, irá dar continuídade ao Hino Nacional Brasileiro.\n",
    "Se não responder, é porque o modelo não tem conhecimento do hino, ou algum problema de configuração\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Carrega as variáveis de ambiente do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "llm = AzureOpenAI()\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT'] \n",
    "\n",
    "\n",
    "# Função para obter uma resposta do modelo\n",
    "# Esta função envia um prompt para o modelo e retorna a resposta\n",
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = llm.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=messages,\n",
    "        temperature=0, #  this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "### 1. Primeiro, defina o texto que você quer usar no prompt\n",
    "# Aqui, vamos usar um trecho do Hino Nacional Brasileiro\n",
    "text = f\"\"\"\n",
    "Ouviram do Ipiranga às margens plácidas\n",
    "\"\"\"\n",
    "\n",
    "### 2. Usar o texto no prompt\n",
    "# O prompt é uma string que será enviada ao modelo\n",
    "# Aqui, estamos formatando o texto dentro de um bloco de código para que o modelo entenda que é um texto a ser completado\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Executar a função para obter a resposta do modelo\n",
    "# A função get_completion envia o prompt para o modelo e retorna a resposta\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo:  Resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "Júpiter é o quinto planeta a partir do Sol e o maior do Sistema Solar.  \\\n",
    "É um gigante gasoso com uma massa equivalente a um milésimo da do Sol,  \\\n",
    "mas duas vezes e meia maior do que a de todos os outros planetas do Sistema Solar juntos. \\\n",
    "Júpiter é um dos objetos mais brilhantes visíveis a olho nu no céu noturno,  \\\n",
    "e é conhecido pelas civilizações antigas desde antes do início da história registrada.  \\\n",
    "Seu nome é uma homenagem ao deus romano Júpiter. Quando visto da Terra,  \\\n",
    "Júpiter pode ser tão brilhante que sua luz refletida é capaz de projetar sombras visíveis,  \\\n",
    "e é, em média, o terceiro objeto natural mais brilhante no céu noturno, depois da Lua e de Vênus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Resuma o conteúdo fornecido para um aluno do terceiro ano.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo: Complemento\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Complete a frase: Era uma vez um\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]  \n",
    "# make completion\n",
    "completion = llm.chat.completions.create(model=deployment, messages=messages)\n",
    "\n",
    "# print response\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo: Complemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Faça perguntas sobre a linguagem SOLID ao seu colega de estudo:\")\n",
    "prompt = f\"\"\"\n",
    "Você é um especialista na linguagem em orientação ao objetos e com amblo conhecimento em SOLID.\n",
    "\n",
    "Sempre que certas perguntas forem feitas, você precisa fornecer uma resposta no formato abaixo.\n",
    "\n",
    "- Conceito\n",
    "- Código de exemplo mostrando a implementação do conceito\n",
    "- Explicação do exemplo e de como o conceito é implementado para melhor compreensão do usuário.\n",
    "\n",
    "Forneça a resposta para a pergunta: {question}\n",
    "\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]  \n",
    "\n",
    "completion = llm.chat.completions.create(model=deployment, messages=messages)\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rodando modelo localmente\n",
    "\n",
    "Vamos agora fazer um teste rodando o ollama local utilizando um container docker\n",
    "\n",
    "\n",
    "```bash\n",
    "\n",
    "docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "docker exec -it ollama bash\n",
    "ollama pull llama3.2\n",
    "\n",
    " \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"prompt\":\"Quem é Linux Torvald?\"}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl http://localhost:11434/api/chat -d '{ \"model\": \"llama3.2\", \"messages\": [ { \"role\": \"user\", \"content\": \"Por que o céu é azul?\" }]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "OllamaLLM(  \n",
    "     base_url=\"http://localhost:11434\",  \n",
    "     model = \"llama3.2\",  # nome do modelo que deseja tem em sua máquina\n",
    "     temperature = 0.3,  \n",
    "     num_predict = 1000, # numero máximo de tokens\n",
    " )\n",
    "\n",
    "template = \"\"\"\n",
    "Você é um especialista em música brasileira e tem amplo conhecimento sobre letras de músicas.\n",
    "\n",
    "Trecho da música: {question}\n",
    "\n",
    "Resposta: Dê continuidade a música e o final conte seu histórico.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model = \"llama3.2\") \n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"question\": \"Ouviram do Ipiranga às margens plácidas\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a versão via chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "chat = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "response = chat.invoke(\"Conte uma piada\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Você é um assistente útil.\"),\n",
    "    HumanMessage(content=\"O que acontece quando uma força imparável encontra um objeto inamovível?\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Você é um assistente útil.\"),\n",
    "    HumanMessage(content=\"Por que o céu é azul?\")\n",
    "]\n",
    "\n",
    "response_stream = chat.stream(messages)\n",
    "for stream in response_stream:\n",
    "    print(stream.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos apagar o container para não ficar ocupando espaço"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "docker container stop ollama\n",
    "docker container rm ollama\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (grassato)",
   "language": "python",
   "name": "grassato"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
