{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Documentação](https://python.langchain.com/docs/how_to/#retrievers)\n",
    "\n",
    "#### 1. O que é um Retriever?\n",
    "\n",
    "Um **Retriever** é um componente responsável por, dado um texto de consulta (query), **retornar uma lista de documentos relevantes**. O conceito de “documentos” pode variar – podem ser pedaços de texto, registros em um banco de dados, páginas da web etc. \n",
    "\n",
    "É importante salientar que um retriever é um `runnable`, ou seja, você pode invocá-lo. Como retorno teremos um `Document`. Cada documento é frequentemente representado por:\n",
    "\n",
    "- **page_content**: o conteúdo do documento (string).\n",
    "- **metadata**: dados adicionais (por exemplo, fonte, autor, ID, data).\n",
    "\n",
    "A interface de retriever no LangChain é bastante simples:\n",
    "\n",
    "1. **Entrada**: uma query (string)\n",
    "2. **Saída**: lista de objetos `Document` relevantes para a query\n",
    "\n",
    "Por trás desse processo, o retriever pode usar qualquer método de busca, desde pesquisas lexicais (BM25, por exemplo) até buscas semânticas em **bancos de dados vetoriais**. O ponto comum é que, para a aplicação final, basta chamar o retriever com a query, e ele entrega os documentos relevantes. Esse é o nível de abstração que o LangChain te entrega.\n",
    "\n",
    "#### 2. Usando um Vector Store como Retriever\n",
    "\n",
    "Hoje em dia, um dos métodos mais populares (e eficientes) de realizar buscas relevantes em textos não-estruturados é por meio de **bancos de dados vetoriais (vector stores)**. A LangChain permite converter esses bancos em retrievers de forma direta.\n",
    "\n",
    "#### 2.1 Criação de um Vector Store Retriever\n",
    "\n",
    "Suponha que já tenhamos um vector store pronto (por exemplo, usando Chroma, Qdrant, Milvus, Pinecone etc.). Para transformar esse vector store em um retriever, basta chamar o método `as_retriever()`:\n",
    "\n",
    "```python\n",
    "vectorstore = MyVectorStore()  # Ex.: Qdrant, Chroma, Pinecone...\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "Com esse `retriever`, podemos chamá-lo de forma padronizada, por exemplo usando `invoke` (ou `.get_relevant_documents(query)`, dependendo da versão da LangChain):\n",
    "\n",
    "```python\n",
    "docs = retriever.invoke(\"Qual foi a declaração do aluno sobre IA?\")\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "```\n",
    "\n",
    "#### 2.2 Personalizando o Tipo de Busca\n",
    "\n",
    "No caso de `vector stores` (banco de dados vetorial), a busca pode variar, ou seja, você pode pre configurar como deve ser a estratégia de similaridade parametrizando o parâmetro `search_type` e `search_kwargs`:\n",
    "\n",
    "- **similarity_search** (padrão): busca documentos com maior similaridade vetorial.\n",
    "- **maximum marginal relevance (mmr)**: procura variar mais os resultados (diversificar). Por exemplo, `search_type=\"mmr\"`\n",
    "- **limite mínimo de score** (threshold): só retorna documentos que atinjam um nível mínimo de similaridade. Por exemplo, `search_type=\"similarity_score_threshold\"`\n",
    "- **top-k**: permite definir quantos documentos retornar. Por exemplo: `search_kwargs={\"k\": 1}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../helpers/00-llm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.llm import initialize_llm, logger, pretty_print_docs\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader , PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "llm, _, embeddings = initialize_llm()\n",
    "\n",
    "NOME_COLECAO=\"FAQ_BOOKING_COM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_documentos = PyPDFLoader('data/FAQ_BOOKING_COM.pdf').load()\n",
    "#lista_documentos = TextLoader('data/FAQ_BOOKING_COM.txt', encoding='utf-8').load() # Caso queira usar o TXT\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\"],  chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "documentos = text_splitter.create_documents(lista_documentos) \n",
    "pretty_print_docs(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o banco de dados vetorial, gerando os embeddings dos documentos\n",
    "Chroma.from_documents(documentos, collection_name=NOME_COLECAO, embedding=embeddings, persist_directory=\"./FAQ_BOOKING_COM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conecta-se ao banco vetorial já existente\n",
    "db = Chroma(  \n",
    "    persist_directory=\"./FAQ_BOOKING_COM\",  \n",
    "    collection_name=NOME_COLECAO,  \n",
    "    embedding_function=embeddings,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banco Vetorial como Recuperador:\n",
    "# db_retriever = db.as_retriever()\n",
    "\n",
    "# Banco Vetorial como Recuperador, configurando parâmetros:\n",
    "#db_retriever = db.as_retriever(search_kwargs={'k': 5, })\n",
    "db_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3, 'fetch_k': 10})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de consulta no banco:\n",
    "query = \"Cancelaram minha reserva o que fazer?\"\n",
    "# query = \"Quando eu chego na hospedagem preciso pagar algo?\"\n",
    "pedacos_retornados = db_retriever.invoke(query)\n",
    "\n",
    "\n",
    "print(f\"Total de pedaços retornados (documents): {len(pedacos_retornados)}\")\n",
    "\n",
    "for i, pedaco in enumerate(pedacos_retornados):\n",
    "    print(f\"------ (documents) chunk {i} -------\")\n",
    "    print(pedaco.page_content)\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banco Vetorial como Recuperador:\n",
    "# db_retriever = db.as_retriever()\n",
    "\n",
    "# Banco Vetorial como Recuperador, configurando parâmetros:\n",
    "db_retriever = db.as_retriever(search_kwargs={'k': 5, })\n",
    "#db_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3, 'fetch_k': 10})\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=db_retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de consulta no banco:\n",
    "# query = \"Cancelaram minha reserva o que fazer?\"\n",
    "query = \"Quando eu chego na hospedagem preciso pagar algo?\"\n",
    "\n",
    "# Executando o recuperador:\n",
    "pedacos_retornados = db_retriever.invoke(query)\n",
    "\n",
    "\n",
    "print(f\"Total de pedaços retornados (documents): {len(pedacos_retornados)}\\n\")\n",
    "\n",
    "for i, pedaco in enumerate(pedacos_retornados):\n",
    "    print(f\"------ (documents) chunk {i} -------\")\n",
    "    print(pedaco.page_content)\n",
    "    print(\"-------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O LLM produz variações da pergunta, por exemplo:\n",
    "\n",
    "- “Como podemos dividir problemas em etapas menores?”\n",
    "- “Quais técnicas de decomposição de tarefas são mais utilizadas?”\n",
    "- etc.\n",
    "\n",
    "Depois, o multi-query faz a busca para cada variação e combina os documentos relevantes, resultando em uma visão **mais abrangente** do assunto.\n",
    "\n",
    "Leia mais em: https://python.langchain.com/docs/how_to/MultiQueryRetriever/\n",
    "\n",
    "### 4.3 Outros\n",
    "\n",
    "Existem outros tipos de retrievers que valem a pena consultar na fonte da documentação:\n",
    "\n",
    "- [Self Query](https://python.langchain.com/docs/how_to/self_query/)\n",
    "- [Contextual Compression](https://python.langchain.com/docs/how_to/contextual_compression/)\n",
    "\n",
    "## 5. Conclusão\n",
    "\n",
    "### 5.1 Por que usar Retrievers?\n",
    "\n",
    "Os **retrievers** desempenham um papel fundamental em aplicações de **IA e NLP** que precisam obter informações relevantes de grandes volumes de dados ou misturar diferentes fontes de conhecimento. A interface padronizada da LangChain:\n",
    "\n",
    "- Simplifica o processo de alternar ou combinar diferentes métodos de busca.\n",
    "- Fornece um ponto único de integração com LLMs para criar **aplicações que buscam e respondem** a perguntas com mais precisão e contexto.\n",
    "\n",
    "### 5.2 Recomendações Finais\n",
    "\n",
    "- Avalie o tipo de dado e o caso de uso para escolher o método de busca.\n",
    "- Se os resultados estiverem muito homogêneos ou superficiais, considere **modelos avançados** (como MMR) ou o uso de **MultiQueryRetriever**.\n",
    "\n",
    "Com esses conceitos, você terá uma base sólida para adicionar **retrieval** (recuperação de informação) em suas aplicações, integrando com LLMs ou outras ferramentas de NLP para obter respostas relevantes e contextualizadas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
