{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ§  LangChain â€” Foco em Componentes\n",
    "LangChain Ã© um framework de orquestraÃ§Ã£o para construir agentes e pipelines com LLMs. Ele oferece uma sÃ©rie de abstraÃ§Ãµes reutilizÃ¡veis, como:\n",
    "\n",
    "LLMs, Prompts, Chains, Tools, Agents, Memory, Retrievers\n",
    "\n",
    "Ele funciona de forma sequencial (ex: LLMChain) ou com agentes que escolhem o prÃ³ximo passo.\n",
    "\n",
    "âœ… Ideal para fluxos lineares ou com lÃ³gica baseada em linguagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ LangGraph â€” Foco em Fluxos de Controle (Graph-Based)\n",
    "LangGraph Ã© uma biblioteca construÃ­da sobre o LangChain para criar fluxos de execuÃ§Ã£o baseados em grafos â€” ou seja, vocÃª modela sua aplicaÃ§Ã£o como nÃ³s (nodes) e arestas (edges), que representam a lÃ³gica de decisÃ£o.\n",
    "\n",
    "âœ… Ideal para construir mÃ¡quinas de estado, agentes multi-etapas, workflows complexos, retries, loops, branching, etc.\n",
    "\n",
    "Conceitos principais:\n",
    "Node = uma funÃ§Ã£o (Runnable) que processa um estado, nÃ³s\n",
    "\n",
    "Edge = transiÃ§Ã£o condicional entre nÃ³s, arestas(realiza conexÃµes entre os nÃ³s)\n",
    "\n",
    "Graph = define como os nÃ³s se conectam\n",
    "\n",
    "State = objeto que carrega informaÃ§Ãµes do processo em andamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../helpers/00-llm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.llm import initialize_llm, logger, pretty_print\n",
    " \n",
    "llm, _, _ = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from typing import TypedDict\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from duckduckgo_search import DDGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    pergunta: str  \n",
    "    conteudo: str \n",
    "    resposta: str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    " \n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            # Ensure you start your weather server on port 8000\n",
    "            \"url\": \"http://127.0.0.1:8000/mcp\",\n",
    "            \"transport\": \"streamable_http\",\n",
    "        },\n",
    "        \"pokemon\": {\n",
    "            # Ensure you start your weather server on port 8080\n",
    "            \"url\": \"http://127.0.0.1:8080/mcp\",\n",
    "            \"transport\": \"streamable_http\",\n",
    "        },\n",
    "          \"math\": {\n",
    "            \"command\": \"python\",\n",
    "            # Make sure to update to the full absolute path to your math_server.py file\n",
    "            \"args\": [\"tools/math_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = await client.get_tools()\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    tools\n",
    ")\n",
    "# math_response = await agent.ainvoke(\n",
    "#     {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n",
    "# )\n",
    "# print(math_response['messages'][-1].content)\n",
    "# weather_response = await agent.ainvoke(\n",
    "#     {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n",
    "# )\n",
    "\n",
    "# print(weather_response['messages'][-1].content)\n",
    " \n",
    "\n",
    "pokemon_response = await agent.ainvoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Ash\"}]}\n",
    ")\n",
    "\n",
    "print(pokemon_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = llm.bind_tools(tools).invoke(state[\"messages\"])\n",
    "    # Suponha que `response` seja o objeto retornado pelo LLM (ChatMessage)\n",
    "    tool_calls = response.additional_kwargs.get(\"tool_calls\", [])\n",
    "\n",
    "    # Itera sobre as chamadas de ferramenta e extrai o nome\n",
    "    for call in tool_calls:\n",
    "        tool_name = call.get(\"function\", {}).get(\"name\")\n",
    "        print(f\"Tool chamada: {tool_name}\")\n",
    "    if not tool_calls:\n",
    "        print(f\"UsuÃ¡rio perguntou: {state['messages'][-1].content}\")     \n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_node(ToolNode(tools))\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_conditional_edges(\n",
    "    \"call_model\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"call_model\")\n",
    "graph = builder.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_response = await graph.ainvoke({\"messages\": \"me de informaÃ§Ãµes sobre pikachu\"})\n",
    "\n",
    "print(math_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pergunta mista, primeiro resolve a soma utilizando IA depois a multiplicaÃ§Ã£o usando o mcp multiplicador\n",
    "math_response = await graph.ainvoke({\"messages\": \"quanto Ã© (3 + 5) x 12?\"})\n",
    "\n",
    "print(math_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_response = await graph.ainvoke({\"messages\": \"what is the weather in nyc?\"},config=config)\n",
    "\n",
    "print(weather_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo o grafo\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "print(graph.get_graph().print_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver, InMemorySaver\n",
    "graph = builder.compile(checkpointer=InMemorySaver())\n",
    "\n",
    "config={\"configurable\":{\"thread_id\":159}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#math_response = await graph.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "weather_response = await graph.ainvoke({\"messages\": \"what is the weather in nyc?\"},config=config)\n",
    "\n",
    "print(weather_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_response = await graph.ainvoke({\"messages\": \"How is the weather in NYC?\"},config=config)\n",
    "\n",
    "print(weather_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FunÃ§Ãµes de CallBack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recebe_pergunta(state: State) -> State:\n",
    "    print(f\"UsuÃ¡rio perguntou: {state['pergunta']}\")\n",
    "    return {\"pergunta\": state[\"pergunta\"]}\n",
    "\n",
    "def precisa_pesquisar(state: State) -> State:\n",
    "    pergunta = state[\"pergunta\"].lower()\n",
    "    precisa = any(p in pergunta for p in [\"tempo\",\"clima\", \"weather\"])\n",
    "    print(\"Precisa pesquisar?\", precisa)\n",
    "    # Retorna um dicionÃ¡rio, com chave especial para decisÃ£o\n",
    "    return {\"next_step\": \"pesquisar\" if precisa else \"consultar_llm\"}\n",
    "\n",
    "def pesquisar(state: State) -> State:\n",
    "    pergunta = state[\"pergunta\"]\n",
    "    print(f\"Pesquisando no DuckDuckGo: {pergunta}\")\n",
    "\n",
    "    with DDGS() as ddgs:\n",
    "        resultados = ddgs.text(pergunta, max_results=1)\n",
    "\n",
    "    if resultados:\n",
    "        contexto = \"\\n\".join([r[\"body\"] for r in resultados if \"body\" in r])\n",
    "    else:\n",
    "        contexto = \"Nenhum resultado encontrado.\"\n",
    "\n",
    "    return {\"conteudo\": contexto}    \n",
    "\n",
    "def consultar_llm(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\"Responda Ã  seguinte pergunta: {pergunta}\")\n",
    "    chain = prompt | llm\n",
    "    resposta = chain.invoke({\"pergunta\": state[\"pergunta\"]})\n",
    "    print(\"Resposta direta do LLM.\")\n",
    "    return {\"resposta\": resposta.content}\n",
    "\n",
    "def sintetizar(state: State) -> State:\n",
    "    contexto = state.get(\"conteudo\", \"\")\n",
    "    pergunta = state[\"pergunta\"]\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Use o seguinte contexto para responder a pergunta:\n",
    "    Contexto: {contexto}\n",
    "    Pergunta: {pergunta}\n",
    "    Resposta:\"\"\")\n",
    "    chain = prompt | llm\n",
    "    resposta = chain.invoke({\"contexto\": contexto, \"pergunta\": pergunta})\n",
    "    print(\"Resposta sintetizada com contexto.\")\n",
    "    return {\"resposta\": resposta.content}\n",
    "\n",
    "def responder(state: State) -> State:\n",
    "    print(\"\\n Resposta Final:\")\n",
    "    print(state[\"resposta\"])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"recebe_pergunta\", RunnableLambda(recebe_pergunta))\n",
    "graph.add_node(\"decisao\", RunnableLambda(precisa_pesquisar))\n",
    "graph.add_node(\"pesquisar\", RunnableLambda(pesquisar))\n",
    "graph.add_node(\"consultar_llm\", RunnableLambda(consultar_llm))\n",
    "graph.add_node(\"sintetizar\", RunnableLambda(sintetizar))\n",
    "graph.add_node(\"responder\", RunnableLambda(responder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransiÃ§Ãµes de Estado e CondiÃ§Ãµes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.set_entry_point(\"recebe_pergunta\")\n",
    "\n",
    "graph.add_edge(\"recebe_pergunta\", \"decisao\")\n",
    "graph.add_conditional_edges(\n",
    "    \"decisao\",\n",
    "    lambda state: state[\"next_step\"],  \n",
    "    {\n",
    "        \"pesquisar\": \"pesquisar\",\n",
    "        \"consultar_llm\": \"consultar_llm\"\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"pesquisar\", \"sintetizar\")\n",
    "graph.add_edge(\"consultar_llm\", \"responder\")\n",
    "graph.add_edge(\"sintetizar\", \"responder\")\n",
    "graph.set_finish_point(\"responder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExecuÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable = graph.compile()\n",
    "\n",
    "print(\"\\n TESTE 1:\")\n",
    "executable.invoke({\"pergunta\": \"Qual Ã© a capital da Alemanha?\"})\n",
    "\n",
    "print(\"\\n  TESTE 2:\")\n",
    "executable.invoke({\"pergunta\": \"Me mostre dados sobre economia brasileira em 2025.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo o grafo\n",
    "display(Image(executable.get_graph().draw_mermaid_png()))\n",
    "\n",
    "print(executable.get_graph().print_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class ChatState(TypedDict):\n",
    "    pergunta: str\n",
    "    resposta: str\n",
    "\n",
    "# FunÃ§Ã£o que responde perguntas\n",
    "def responder(state: ChatState) -> ChatState:\n",
    "    pergunta = state.get(\"pergunta\", \"\")\n",
    "    resposta = llm.invoke(pergunta)\n",
    "    return {\"pergunta\": pergunta, \"resposta\": resposta.content}\n",
    "\n",
    "# Construindo o grafo\n",
    "builder = StateGraph(ChatState)\n",
    " \n",
    "builder.add_node(\"responder\", responder)\n",
    "\n",
    "builder.set_entry_point(\"responder\")\n",
    "builder.add_edge(\"responder\", END)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilando o grafo\n",
    "graph = builder.compile()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo o grafo\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "print(graph.get_graph().print_ascii())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"Qual a capital do Brasil?\"\n",
    "\n",
    "# Criando o agente\n",
    "entrada = ChatState({\"pergunta\": pergunta})\n",
    "resultado = graph.invoke(entrada)\n",
    "\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
